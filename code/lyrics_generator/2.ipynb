{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv(\"../data/EdmondsDance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_labels = np.array([\n",
    "            \"Joy\",\n",
    "            \"Trust\",\n",
    "            \"Fear\",\n",
    "            \"Surprise\",\n",
    "            \"Sadness\",\n",
    "            \"Disgust\",\n",
    "            \"Anger\",\n",
    "            \"Anticipation\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Song</th>\n",
       "      <th>Artists</th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Trust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Anticipation</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Apollo</td>\n",
       "      <td>Hardwell, Amba Shepherd</td>\n",
       "      <td>Just one day in the life&lt;br&gt;So I can understan...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Lullaby</td>\n",
       "      <td>R3HAB, Mike Williams</td>\n",
       "      <td>Hypnotized, this love out of me&lt;br&gt;Without you...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Melody (Tip Of My Tongue)</td>\n",
       "      <td>Mike Williams</td>\n",
       "      <td>I stand a little too close&lt;br&gt;You stare a litt...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Take Me Home</td>\n",
       "      <td>Cash Cash, Bebe Rexha</td>\n",
       "      <td>I'm falling to pieces&lt;br&gt;But I need this&lt;br&gt;Ye...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>City of Dreams</td>\n",
       "      <td>Dirty South, Alesso</td>\n",
       "      <td>Everything seems like a city of dreams,&lt;br&gt;I n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                       Song                  Artists  \\\n",
       "0           0                     Apollo  Hardwell, Amba Shepherd   \n",
       "1           1                    Lullaby     R3HAB, Mike Williams   \n",
       "2           2  Melody (Tip Of My Tongue)            Mike Williams   \n",
       "3           3               Take Me Home    Cash Cash, Bebe Rexha   \n",
       "4           4             City of Dreams     Dirty South, Alesso    \n",
       "\n",
       "                                              Lyrics  Joy  Trust  Fear  \\\n",
       "0  Just one day in the life<br>So I can understan...    1      1     0   \n",
       "1  Hypnotized, this love out of me<br>Without you...    0      0     1   \n",
       "2  I stand a little too close<br>You stare a litt...    1      1     0   \n",
       "3  I'm falling to pieces<br>But I need this<br>Ye...    0      0     0   \n",
       "4  Everything seems like a city of dreams,<br>I n...    0      0     0   \n",
       "\n",
       "   Surprise  Sadness  Disgust  Anger  Anticipation  Unnamed: 11  \n",
       "0         1        0        0      0             0          NaN  \n",
       "1         0        1        0      0             0          NaN  \n",
       "2         0        0        0      0             1          NaN  \n",
       "3         1        1        1      0             0          NaN  \n",
       "4         1        1        0      0             0          NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = raw_df[raw_df[\"Sadness\"] == 1][\"Lyrics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Song</th>\n",
       "      <th>Artists</th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Trust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Anticipation</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Lullaby</td>\n",
       "      <td>R3HAB, Mike Williams</td>\n",
       "      <td>Hypnotized, this love out of me&lt;br&gt;Without you...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Take Me Home</td>\n",
       "      <td>Cash Cash, Bebe Rexha</td>\n",
       "      <td>I'm falling to pieces&lt;br&gt;But I need this&lt;br&gt;Ye...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>City of Dreams</td>\n",
       "      <td>Dirty South, Alesso</td>\n",
       "      <td>Everything seems like a city of dreams,&lt;br&gt;I n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Let You Go</td>\n",
       "      <td>The Chainsmokers, Great Good Fine Ok</td>\n",
       "      <td>You end up alone after all that you've done&lt;br...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>This is What It Feels Like</td>\n",
       "      <td>Armin Van Buuren, Trevor Guthrie</td>\n",
       "      <td>Nobody here knocking at my door&lt;br&gt;The sound o...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>511</td>\n",
       "      <td>Over My Head (Remix)</td>\n",
       "      <td>Echosmith, Moti</td>\n",
       "      <td>Over my head, over my head, over my head&lt;br&gt;Ov...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>514</td>\n",
       "      <td>Faking It</td>\n",
       "      <td>Lost Stories</td>\n",
       "      <td>Cause I've been faking&lt;br&gt;Faking it for too lo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>521</td>\n",
       "      <td>Chicago (Remix)</td>\n",
       "      <td>Win and Woo, Bryce Fox, SHADES</td>\n",
       "      <td>There's not enough room in here&lt;br&gt;For room fo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>522</td>\n",
       "      <td>Haunted</td>\n",
       "      <td>PATAY</td>\n",
       "      <td>I see you everywhere&lt;br&gt;I never moved on&lt;br&gt;Wi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>523</td>\n",
       "      <td>Firefly</td>\n",
       "      <td>Miles Away, Amanda Yang</td>\n",
       "      <td>These stars are, really fireflies that lost th...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>185 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                        Song  \\\n",
       "1             1                     Lullaby   \n",
       "3             3                Take Me Home   \n",
       "4             4              City of Dreams   \n",
       "5             5                 Let You Go    \n",
       "6             6  This is What It Feels Like   \n",
       "..          ...                         ...   \n",
       "511         511        Over My Head (Remix)   \n",
       "514         514                   Faking It   \n",
       "521         521             Chicago (Remix)   \n",
       "522         522                     Haunted   \n",
       "523         523                     Firefly   \n",
       "\n",
       "                                  Artists  \\\n",
       "1                    R3HAB, Mike Williams   \n",
       "3                   Cash Cash, Bebe Rexha   \n",
       "4                    Dirty South, Alesso    \n",
       "5    The Chainsmokers, Great Good Fine Ok   \n",
       "6        Armin Van Buuren, Trevor Guthrie   \n",
       "..                                    ...   \n",
       "511                       Echosmith, Moti   \n",
       "514                          Lost Stories   \n",
       "521        Win and Woo, Bryce Fox, SHADES   \n",
       "522                                 PATAY   \n",
       "523               Miles Away, Amanda Yang   \n",
       "\n",
       "                                                Lyrics  Joy  Trust  Fear  \\\n",
       "1    Hypnotized, this love out of me<br>Without you...    0      0     1   \n",
       "3    I'm falling to pieces<br>But I need this<br>Ye...    0      0     0   \n",
       "4    Everything seems like a city of dreams,<br>I n...    0      0     0   \n",
       "5    You end up alone after all that you've done<br...    0      0     0   \n",
       "6    Nobody here knocking at my door<br>The sound o...    0      0     1   \n",
       "..                                                 ...  ...    ...   ...   \n",
       "511  Over my head, over my head, over my head<br>Ov...    0      0     0   \n",
       "514  Cause I've been faking<br>Faking it for too lo...    0      1     0   \n",
       "521  There's not enough room in here<br>For room fo...    0      0     0   \n",
       "522  I see you everywhere<br>I never moved on<br>Wi...    0      0     0   \n",
       "523  These stars are, really fireflies that lost th...    1      1     0   \n",
       "\n",
       "     Surprise  Sadness  Disgust  Anger  Anticipation  Unnamed: 11  \n",
       "1           0        1        0      0             0          NaN  \n",
       "3           1        1        1      0             0          NaN  \n",
       "4           1        1        0      0             0          NaN  \n",
       "5           1        1        1      1             0          NaN  \n",
       "6           0        1        0      0             0          NaN  \n",
       "..        ...      ...      ...    ...           ...          ...  \n",
       "511         1        1        0      1             0          NaN  \n",
       "514         0        1        0      0             0          NaN  \n",
       "521         1        1        0      0             0          NaN  \n",
       "522         0        1        0      0             0          NaN  \n",
       "523         0        1        0      0             0          NaN  \n",
       "\n",
       "[185 rows x 13 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " raw_df[raw_df[\"Sadness\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hypnotized, this love out of me \\n Without your air I can't even breathe \\n Lead my way out into the light \\n Sing your lullaby \\n Cherries in the ashtray \\n Take me through the day \\n I just gotta make you drunk in memory \\n See you in the puddles \\n Of my Chardonnay \\n Sleeping in my bathtub \\n But can wish you were late \\n Keep me safe up in the clouds \\n 'Cause I can't come raining down \\n Make the monsters sleep in my mind \\n Sing your lullaby \\n Hypnotized, this love out of me \\n Without your air I can't even breathe \\n Lead my way out into the light \\n Sing your lullaby \\n Cherries in the ashtray \\n Take me through the day \\n I just gotta make you drunk in memory \\n See you in the puddles \\n Of my Chardonnay \\n Sleeping in my bathtub \\n But can wish you were late \\n Keep me safe up in the clouds \\n 'Cause I can't come raining down \\n Make the monsters sleep in my mind \\n Sing your lullaby\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.iloc[0].replace(\"<br>\", \" \\n \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creat_corpus(raw_df):\n",
    "    corpus = {}\n",
    "    for emotion_label in emotion_labels:\n",
    "        lyrics_series = raw_df[raw_df[emotion_label] == 1][\"Lyrics\"]\n",
    "        corpus[emotion_label] = list(lyrics_series)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_corpus = creat_corpus(raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\shivs\\anaconda3\\envs\\chess-engine\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3789\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3791\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'labels'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sad_song_series \u001b[38;5;241m=\u001b[39m raw_df[\u001b[43mraw_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSadness\u001b[39m\u001b[38;5;124m\"\u001b[39m)][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlyrics\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\shivs\\anaconda3\\envs\\chess-engine\\lib\\site-packages\\pandas\\core\\frame.py:3896\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3895\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3896\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3898\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\shivs\\anaconda3\\envs\\chess-engine\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3793\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3795\u001b[0m     ):\n\u001b[0;32m   3796\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3797\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3799\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'labels'"
     ]
    }
   ],
   "source": [
    "sad_song_series = raw_df[raw_df[\"labels\"].str.contains(\"Sadness\")][\"lyrics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sad_song_series = [[\"\"\"I walk a lonely road\n",
    "The only one that I have ever known\n",
    "Don't know where it goes\n",
    "But it's home to me, and I walk alone\n",
    "I walk this empty street\n",
    "On the Boulevard of Broken Dreams\n",
    "Where the city sleeps\n",
    "And I'm the only one, and I walk alone\n",
    "I walk alone, I walk alone\n",
    "I walk alone, I walk a-\n",
    "My shadow's the only one that walks beside me\n",
    "My shallow heart's the only thing that's beating\n",
    "Sometimes, I wish someone out there will find me\n",
    "'Til then, I walk alone\n",
    "Ah-ah, ah-ah, ah-ah, ah-ah\n",
    "Ah-ah, ah-ah, ah-ah\n",
    "I'm walking down the line\n",
    "That divides me somewhere in my mind\n",
    "On the borderline\n",
    "Of the edge, and where I walk alone\n",
    "Read between the lines\n",
    "What's fucked up, and everything's alright\n",
    "Check my vital signs\n",
    "To know I'm still alive, and I walk alone\n",
    "I walk alone, I walk alone\n",
    "I walk alone, I walk a-\n",
    "My shadow's the only one that walks beside me\n",
    "My shallow heart's the only thing that's beating\n",
    "Sometimes, I wish someone out there will find me\n",
    "'Til then, I walk alone\n",
    "Ah-ah, ah-ah, ah-ah, ah-ah\n",
    "Ah-ah, ah-ah, I walk alone, I walk a-\n",
    "I walk this empty street\n",
    "On the Boulevard of Broken Dreams\n",
    "Where the city sleeps\n",
    "And I'm the only one, and I walk a-\n",
    "My shadow's the only one that walks beside me\n",
    "My shallow heart's the only thing that's beating\n",
    "Sometimes, I wish someone out there will find me\n",
    "'Til then, I walk alone\"\"\"], [\"\"\"\n",
    "I've got a really bad disease\n",
    "It's got me begging on my hands and knees\n",
    "So take me to emergency\n",
    "'Cause something seems to be missing\n",
    "Somebody take the pain away\n",
    "It's like an ulcer bleeding in my brain\n",
    "So send me to the pharmacy\n",
    "So I can lose my memory\n",
    "I'm elated, medicated\n",
    "Lord knows I've tried to find a way\n",
    "To run away\n",
    "I think they found another cure\n",
    "For broken hearts and feeling insecure\n",
    "You'd be surprised what I endure\n",
    "What makes you feel so self-assured?\n",
    "I need to find a place to hide\n",
    "You never know what could be waiting outside\n",
    "The accidents that you could find\n",
    "It's like some kind of suicide\n",
    "So what ails you is what impales you?\n",
    "I feel like I've been crucified\n",
    "To be satisfied\n",
    "I'm a victim of my symptom\n",
    "I am my own worst enemy\n",
    "You're a victim of your symptom\n",
    "You are your own worst enemy\n",
    "Know your enemy\n",
    "I'm elated, medicated\n",
    "I am my own worst enemy\n",
    "So what ails you is what impales you?\n",
    "You are your own worst enemy\n",
    "You're a victim of the system\n",
    "You are your own worst enemy\n",
    "You're a victim of the system\n",
    "You are your own worst enemy\n",
    "\"\"\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sad_song_series' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m sad_song_corpus \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m song \u001b[38;5;129;01min\u001b[39;00m \u001b[43msad_song_series\u001b[49m[:\u001b[38;5;241m30\u001b[39m]:\n\u001b[0;32m      3\u001b[0m     sad_song_corpus\u001b[38;5;241m.\u001b[39mappend(song)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sad_song_series' is not defined"
     ]
    }
   ],
   "source": [
    "sad_song_corpus = []\n",
    "for song in sad_song_series[:30]:\n",
    "    sad_song_corpus.append(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sad_song_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk  # just for tokenization\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "\n",
    "def tokenize(lyrics: list[str]) -> list[list[str]]:\n",
    "    result = []\n",
    "    for lyric in tqdm(lyrics):\n",
    "        lyric.replace(\"<b>\", \" \\n \")\n",
    "        # lowercase the text,punctuation and keep only the words\n",
    "        tokens = nltk.tokenize.word_tokenize(lyric.lower())\n",
    "        stop_words = list(string.punctuation)\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        alpha_tokens = [\n",
    "            lemmatizer.lemmatize(token)\n",
    "            for token in tokens\n",
    "        ]\n",
    "        result.append(alpha_tokens)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_token_corpus(corpus):\n",
    "    token_corpus = {}\n",
    "    for emotion, lyrics in corpus.items():\n",
    "        tokens = tokenize(lyrics)\n",
    "        token_corpus[emotion] = tokens\n",
    "    return token_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/230 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 230/230 [00:02<00:00, 114.29it/s]\n",
      "100%|██████████| 294/294 [00:00<00:00, 490.49it/s]\n",
      "100%|██████████| 103/103 [00:00<00:00, 538.23it/s]\n",
      "100%|██████████| 68/68 [00:00<00:00, 544.31it/s]\n",
      "100%|██████████| 185/185 [00:00<00:00, 529.76it/s]\n",
      "100%|██████████| 115/115 [00:00<00:00, 364.51it/s]\n",
      "100%|██████████| 72/72 [00:00<00:00, 389.93it/s]\n",
      "100%|██████████| 249/249 [00:00<00:00, 563.15it/s]\n"
     ]
    }
   ],
   "source": [
    "token_corpus = create_token_corpus(song_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "class word2vec():\n",
    "    def __init__ (self):\n",
    "        self.n = settings['n']\n",
    "        self.eta = settings['learning_rate']\n",
    "        self.epochs = settings['epochs']\n",
    "        self.window = settings['window_size']\n",
    "    \n",
    "    \n",
    "    def generate_training_data(self, settings, corpus):\n",
    "\n",
    "        word_counts = defaultdict(int)\n",
    "        for row in corpus:\n",
    "            for word in row:\n",
    "                word_counts[word] += 1\n",
    "\n",
    "        self.v_count = len(word_counts.keys())\n",
    "\n",
    "        self.words_list = sorted(list(word_counts.keys()),reverse=False)\n",
    "        self.word_index = dict((word, i) for i, word in enumerate(self.words_list))\n",
    "        self.index_word = dict((i, word) for i, word in enumerate(self.words_list))\n",
    "\n",
    "        training_data = []\n",
    "        for sentence in tqdm(corpus):\n",
    "            sent_len = len(sentence)\n",
    "\n",
    "            \n",
    "            for i, word in enumerate(sentence):\n",
    "                w_target = self.word2onehot(sentence[i])\n",
    "                w_context = []\n",
    "                for j in range(i-self.window, i+self.window+1):\n",
    "                    if j!=i and j<=sent_len-1 and j>=0:\n",
    "                        w_context.append(self.word2onehot(sentence[j]))\n",
    "                training_data.append([w_target, w_context])\n",
    "        \n",
    "        return training_data\n",
    "\n",
    "\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "\n",
    "\n",
    "    \n",
    "    def word2onehot(self, word):\n",
    "        word_vec = [0 for i in range(0, self.v_count)]\n",
    "        word_index = self.word_index[word]\n",
    "        word_vec[word_index] = 1\n",
    "        return word_vec\n",
    "\n",
    "\n",
    "    \n",
    "    def forward_pass(self, x):\n",
    "        h = np.dot(self.w1.T, x)\n",
    "        u = np.dot(self.w2.T, h)\n",
    "        y_c = self.softmax(u)\n",
    "        return y_c, h, u\n",
    "                \n",
    "\n",
    "    \n",
    "    def backprop(self, e, h, x):\n",
    "        dl_dw2 = np.outer(h, e)  \n",
    "        dl_dw1 = np.outer(x, np.dot(self.w2, e.T))\n",
    "\n",
    "        \n",
    "        self.w1 = self.w1 - (self.eta * dl_dw1)\n",
    "        self.w2 = self.w2 - (self.eta * dl_dw2)\n",
    "\n",
    "\n",
    "    \n",
    "    def train(self, training_data):\n",
    "        \n",
    "        limit1 = np.sqrt(2 / float(self.n + self.v_count))\n",
    "        self.w1 = np.random.normal(\n",
    "                0.0, limit1, size=(self.v_count, self.n)\n",
    "            )     # embedding matrix\n",
    "        self.w2 = np.random.normal(\n",
    "                0.0, limit1, size=(self.n, self.v_count)\n",
    "            ) \n",
    "        \n",
    "        \n",
    "        for i in tqdm(range(0, self.epochs)):\n",
    "\n",
    "            self.loss = 0\n",
    "\n",
    "            \n",
    "            for w_t, w_c in training_data:\n",
    "\n",
    "                \n",
    "                y_pred, h, u = self.forward_pass(w_t)\n",
    "                \n",
    "                \n",
    "                EI = np.sum([np.subtract(y_pred, word) for word in w_c], axis=0)\n",
    "\n",
    "                \n",
    "                self.backprop(EI, h, w_t)\n",
    "\n",
    "                \n",
    "                self.loss += -np.sum([u[word.index(1)] for word in w_c]) + len(w_c) * np.log(np.sum(np.exp(u)))\n",
    "                \n",
    "            print('EPOCH:',i, 'LOSS:', self.loss)\n",
    "\n",
    "\n",
    "    \n",
    "    def word_vec(self, word):\n",
    "        w_index = self.word_index[word]\n",
    "        v_w = self.w1[w_index]\n",
    "        return v_w\n",
    "\n",
    "\n",
    "    \n",
    "    def vec_sim(self, vec, top_n):\n",
    "\n",
    "        \n",
    "        word_sim = {}\n",
    "        for i in range(self.v_count):\n",
    "            v_w2 = self.w1[i]\n",
    "            theta_num = np.dot(vec, v_w2)\n",
    "            theta_den = np.linalg.norm(vec) * np.linalg.norm(v_w2)\n",
    "            theta = theta_num / theta_den\n",
    "\n",
    "            word = self.index_word[i]\n",
    "            word_sim[word] = theta\n",
    "\n",
    "        words_sorted = sorted(word_sim.items(), key=lambda item:item[1], reverse=True)\n",
    "\n",
    "        return words_sorted\n",
    "            \n",
    "        \n",
    "\n",
    "    \n",
    "    def word_sim(self, word, top_n):\n",
    "        \n",
    "        w1_index = self.word_index[word]\n",
    "        v_w1 = self.w1[w1_index]\n",
    "\n",
    "        \n",
    "        word_sim = {}\n",
    "        for i in range(self.v_count):\n",
    "            v_w2 = self.w1[i]\n",
    "            theta_num = np.dot(v_w1, v_w2)\n",
    "            theta_den = np.linalg.norm(v_w1) * np.linalg.norm(v_w2)\n",
    "            theta = theta_num / theta_den\n",
    "\n",
    "            word = self.index_word[i]\n",
    "            word_sim[word] = theta\n",
    "\n",
    "        words_sorted = sorted(word_sim.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "        words = []\n",
    "        for word in words_sorted[:top_n]:\n",
    "            words.append(word[0])\n",
    "\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Joy', 'Trust', 'Fear', 'Surprise', 'Sadness', 'Disgust', 'Anger', 'Anticipation'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_corpus.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_corpus[\"Joy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w2v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mw2v\u001b[49m\u001b[38;5;241m.\u001b[39mv_count\n",
      "\u001b[1;31mNameError\u001b[0m: name 'w2v' is not defined"
     ]
    }
   ],
   "source": [
    "w2v.v_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'stand',\n",
       " 'a',\n",
       " 'little',\n",
       " 'too',\n",
       " 'close',\n",
       " '<',\n",
       " 'br',\n",
       " '>',\n",
       " 'you',\n",
       " 'stare',\n",
       " 'a',\n",
       " 'little',\n",
       " 'too',\n",
       " 'long',\n",
       " '<',\n",
       " 'br',\n",
       " '>',\n",
       " 'we',\n",
       " 'do',\n",
       " 'this',\n",
       " 'dance',\n",
       " 'every',\n",
       " 'time',\n",
       " '<',\n",
       " 'br',\n",
       " '>',\n",
       " 'you',\n",
       " 'and',\n",
       " 'i',\n",
       " ',',\n",
       " 'you',\n",
       " 'and',\n",
       " 'i',\n",
       " '<',\n",
       " 'br',\n",
       " '>',\n",
       " '<',\n",
       " 'br',\n",
       " '>',\n",
       " 'yeah',\n",
       " ',',\n",
       " 'we',\n",
       " \"'re\",\n",
       " 'up',\n",
       " 'in',\n",
       " 'the',\n",
       " 'air',\n",
       " '<',\n",
       " 'br',\n",
       " '>',\n",
       " 'like',\n",
       " 'the',\n",
       " 'smoke',\n",
       " 'from',\n",
       " 'your',\n",
       " 'lip',\n",
       " '<',\n",
       " 'br',\n",
       " '>',\n",
       " 'but',\n",
       " 'i',\n",
       " 'ca',\n",
       " \"n't\",\n",
       " 'find',\n",
       " 'my',\n",
       " 'word',\n",
       " '<',\n",
       " 'br',\n",
       " '>',\n",
       " 'when',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'tracing',\n",
       " 'your',\n",
       " 'kiss',\n",
       " 'i',\n",
       " 'can',\n",
       " 'taste',\n",
       " ',',\n",
       " 'i',\n",
       " 'can',\n",
       " 'taste',\n",
       " ',',\n",
       " 'i',\n",
       " 'can',\n",
       " 'taste',\n",
       " 'it',\n",
       " '<',\n",
       " 'br',\n",
       " '>',\n",
       " 'in',\n",
       " 'my',\n",
       " 'mind',\n",
       " ',',\n",
       " 'all',\n",
       " 'the',\n",
       " 'time',\n",
       " ',',\n",
       " 'can',\n",
       " 'you',\n",
       " 'race',\n",
       " 'it',\n",
       " '<',\n",
       " 'br',\n",
       " '>',\n",
       " 'what',\n",
       " 'i',\n",
       " 'do',\n",
       " ',',\n",
       " 'for',\n",
       " 'a',\n",
       " 'shot',\n",
       " 'of',\n",
       " 'your',\n",
       " 'love',\n",
       " '<',\n",
       " 'br',\n",
       " '>',\n",
       " 'i',\n",
       " 'have',\n",
       " 'you',\n",
       " '<',\n",
       " 'br',\n",
       " '>',\n",
       " 'i',\n",
       " 'have',\n",
       " 'you',\n",
       " 'on',\n",
       " 'the',\n",
       " 'tip',\n",
       " 'of',\n",
       " 'my',\n",
       " 'tongue',\n",
       " '<',\n",
       " 'br',\n",
       " '>',\n",
       " 'i',\n",
       " 'have',\n",
       " 'you',\n",
       " 'on',\n",
       " 'the',\n",
       " 'tip',\n",
       " 'of',\n",
       " 'my',\n",
       " 'tongue']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:07<00:00,  6.55it/s]\n",
      " 10%|█         | 1/10 [00:10<01:34, 10.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 LOSS: 510613.2790160722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:20<01:23, 10.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1 LOSS: 442425.69269344665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:31<01:13, 10.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 2 LOSS: 429071.3327638874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:42<01:03, 10.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 3 LOSS: 422537.5999847193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:52<00:52, 10.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 4 LOSS: 418140.476893963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [01:03<00:42, 10.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 5 LOSS: 415129.79603837634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [01:13<00:31, 10.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 6 LOSS: 413056.33313176973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [01:23<00:20, 10.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 7 LOSS: 411556.4504542109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [01:34<00:10, 10.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 8 LOSS: 410443.412793524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:45<00:00, 10.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 9 LOSS: 409591.19756881683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "settings = {}\n",
    "settings['n'] = 5                   # dimension of word embeddings\n",
    "settings['window_size'] = 2         # context window +/- center word\n",
    "settings['min_count'] = 0           # minimum word count\n",
    "settings['epochs'] = 10       # number of training epochs\n",
    "settings['neg_samp'] = 10           # number of negative words to use during training\n",
    "settings['learning_rate'] = 0.01    # learning rate\n",
    "np.random.seed(0)                   # set the seed for reproducibility\n",
    "\n",
    "corpus = token_corpus[\"Sadness\"][:50]\n",
    "\n",
    "w2v = word2vec()\n",
    "\n",
    "# generate training data\n",
    "training_data = w2v.generate_training_data(settings, corpus)\n",
    "\n",
    "# train word2vec model\n",
    "w2v.train(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 123.68it/s]\n"
     ]
    }
   ],
   "source": [
    "n_words = 100\n",
    "previous_word = initial_token = \"we\"\n",
    "predicted_lyrics = initial_token\n",
    "previous_five_words = [initial_token]\n",
    "\n",
    "for _ in tqdm(range(n_words)):\n",
    "    count = 0\n",
    "    tentaitve_next_words = w2v.word_sim(previous_word, 10)\n",
    "    \n",
    "    tentaitve_next_word = np.random.choice(tentaitve_next_words + [\",\",\".\"])\n",
    "    if tentaitve_next_word in [\",\",\".\"]:\n",
    "        previous_five_words.append(tentaitve_next_word)\n",
    "        predicted_lyrics += f\" {tentaitve_next_word}\"\n",
    "\n",
    "    else:\n",
    "        while count < 10 and (tentaitve_next_word in previous_five_words):\n",
    "            tentaitve_next_word = tentaitve_next_words[count]\n",
    "            count += 1\n",
    "\n",
    "        predicted_lyrics += f\" {tentaitve_next_word}\"\n",
    "        if len(previous_five_words) == 5:\n",
    "            previous_five_words.pop(0)\n",
    "        \n",
    "        previous_word = tentaitve_next_word\n",
    "        previous_five_words.append(previous_word)\n",
    "    # if tentaitve_next_word  not in [\",\", \".\"]:\n",
    "    #     previous_five_words.append(previous_word)\n",
    "    #     previous_word = tentaitve_next_word\n",
    "    # else:\n",
    "    #     previous_word = previous_five_words[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"we they since 'cause thinking thought took somebody reflection keep lost hit tarot wish , . call zero angel shook , unfair learn chill bridge use avicii hearing relax , estranged stack till y'all get searching safe truth front tree body . hide mics strong new , , pill pusher cocaine fade border skin thick ariel highest . sinner scarecrow , past make-up breathing fate coming beat fool ground waste prove party bone sympony puddle base zone ashtray chardonnay taxi suburb , concrete haze tape system , . daily . south knee line land , ex . , ya chart trace\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1331, 5)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.w1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.64879883,  1.75212071, -0.73838378,  0.54868277, -0.42380714])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.w1[582]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"w1.npy\", w2v.w1)\n",
    "np.save(\"w2.npy\", w2v.w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"word_index.npy\", w2v.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.load(\"word_index.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_index.json', 'w') as json_file:\n",
    "    json.dump(w2v.word_index, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1331"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.v_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('index_word.json', 'w') as json_file:\n",
    "    json.dump(w2v.index_word, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chess-engine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
