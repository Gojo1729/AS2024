{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import nltk # just for tokenization\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestones\n",
    "1. Get the word2vec and glove vectors\n",
    "2. Tokenize the lyrics corpus\n",
    "3. Get the unique tokens and index them (not needed, may be)\n",
    "4. Get word embeddings of each token in the lyrics, and do linear superposition every word vector to create a single vector for a lyric. Do this for all lyrics\n",
    "5. Use clustering techniques \n",
    "    - Start with Kmeans for setting the baseline then try out other clustering techniques like DBSCAN.\n",
    "6. Calculate the Silhoutte distance, WCSS for the model for comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Glove vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
      "Loaded vocab size 3000000\n"
     ]
    }
   ],
   "source": [
    "def load_embedding_model():\n",
    "    \"\"\" Load GloVe Vectors\n",
    "        Return:\n",
    "            wv_from_bin: All 400000 embeddings, each lengh 200\n",
    "    \"\"\"\n",
    "    import gensim.downloader as api\n",
    "    wv_from_bin = api.load(\"word2vec-google-news-300\")\n",
    "    print(\"Loaded vocab size %i\" % len(list(wv_from_bin.index_to_key)))\n",
    "    return wv_from_bin\n",
    "wv_from_bin = wv_from_bin if (wv_from_bin!=None) else load_embedding_model()\n",
    "# wv_from_bin = load_embedding_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize the lyrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv(\"../../Spotify Song Dataset.csv/million_songs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  artist                   song                                        link  \\\n",
      "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
      "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
      "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
      "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
      "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
      "\n",
      "                                                text  \n",
      "0  Look at her face, it's a wonderful face  \\nAnd...  \n",
      "1  Take it easy with me, please  \\nTouch me gentl...  \n",
      "2  I'll never know why I had to go  \\nWhy I had t...  \n",
      "3  Making somebody happy is a question of give an...  \n",
      "4  Making somebody happy is a question of give an...  \n",
      "Number of songs 57650\n"
     ]
    }
   ],
   "source": [
    "print(raw_df.head())\n",
    "print(f\"Number of songs {len(raw_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57650"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44824"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_df[\"song\"].unique()) # Name of the songs are not unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lyric: str) -> list[str]:\n",
    "    # lowercase the text, remove stop words, punctuation and keep only the words\n",
    "    tokens = nltk.tokenize.word_tokenize(lyric.lower())\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    alpha_tokens = [token for token in tokens if (token.isalpha() and token not in stop_words)]\n",
    "    return alpha_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57650/57650 [01:46<00:00, 541.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# go through each lyrics, tokenize it, vectorize each word, then combine all of them into single average vector and store it in the list\n",
    "spacy_tokenizer = spacy.load(\"en_core_web_sm\")\n",
    "lyrics = raw_df[\"text\"]\n",
    "lyrics_embeddings = []\n",
    "unsupported_tokens = set()\n",
    "for lyric in tqdm(lyrics):\n",
    "    lyric_vector = np.zeros(300)\n",
    "    for token in tokenize(lyric):\n",
    "        try:\n",
    "            lyric_vector += wv_from_bin.get_vector(token.lower())\n",
    "        except KeyError as e:\n",
    "            # if the word is not present in the glove then key error is raised, so handle the exception and move on\n",
    "            unsupported_tokens.add(token)\n",
    "            continue\n",
    "    lyrics_embeddings.append(lyric_vector)\n",
    "\n",
    "lyrics_embeddings = np.stack(lyrics_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape (57650, 300), number of unsupported tokens 32513\n"
     ]
    }
   ],
   "source": [
    "print(f\"Embeddings shape {lyrics_embeddings.shape}, number of unsupported tokens {len(unsupported_tokens)}\")\n",
    "# scaling along each lyrics vector, for unit variance\n",
    "scaled_lyrics_embeddings = lyrics_embeddings / np.linalg.norm(lyrics_embeddings, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chess-engine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
