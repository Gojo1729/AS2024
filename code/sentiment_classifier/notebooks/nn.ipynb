{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import nltk # just for tokenization\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred, num_classes):\n",
    "    # Initialize arrays to store true positives, false positives, and precision\n",
    "    TP = np.zeros(num_classes)\n",
    "    FP = np.zeros(num_classes)\n",
    "    precision_scores = np.zeros(num_classes)\n",
    "\n",
    "    # Calculate true positives and false positives for each class\n",
    "    for i in range(num_classes):\n",
    "        TP[i] = np.sum((y_true == i) & (y_pred == i))\n",
    "        FP[i] = np.sum((y_true != i) & (y_pred == i))\n",
    "\n",
    "    # Compute precision for each class\n",
    "    for i in range(num_classes):\n",
    "        if TP[i] + FP[i] > 0:\n",
    "            precision_scores[i] = TP[i] / (TP[i] + FP[i])\n",
    "\n",
    "    return np.mean(precision_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_loss(y_true, y_pred):\n",
    "    # Calculate number of mismatches\n",
    "    num_mismatches = np.sum(y_true != y_pred)\n",
    "\n",
    "    # Compute Hamming Loss\n",
    "    hamming_loss = num_mismatches / (y_true.shape[0] * y_true.shape[1])\n",
    "\n",
    "    return hamming_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top2_accuracy(predicted_probs, true_labels):\n",
    "\n",
    "    sorted_indices = np.argsort(predicted_probs, axis=1)[:, ::-1]\n",
    "\n",
    "    # Check if true labels are in top-3 predicted labels\n",
    "    top3_correct = np.any(true_labels[np.arange(len(true_labels))[:, None], sorted_indices[:, :2]], axis=1)\n",
    "    # Calculate top-3 accuracy\n",
    "    top3_accuracy = np.mean(top3_correct)\n",
    "    \n",
    "    return top3_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, raw_data, embeddings):\n",
    "        \n",
    "        self.raw_data = raw_data\n",
    "        self.random_state = 42\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = None, None, None, None\n",
    "        self.labels = ['Joy', 'Trust', 'Fear', 'Surprise','Sadness', 'Disgust', 'Anger', 'Anticipation']\n",
    "        self.emotions_onehot = np.array(raw_data.loc[:, self.labels])\n",
    "        self.__pre_process(embeddings)\n",
    "        \n",
    "        self.n_classes = self.y_train.shape[1]\n",
    "        self.n_input_features = self.X_train.shape[1]\n",
    "        self.n_hidden_neurons = 3\n",
    "        \n",
    "        \n",
    "        #weights from input layer to hidden layer1\n",
    "        np.random.seed(self.random_state)\n",
    "        self.W01 = np.random.randn(self.n_input_features, self.n_hidden_neurons)\n",
    "        self.W12 = np.random.randn(self.n_hidden_neurons, self.n_classes)\n",
    "        \n",
    "        self.b01 = np.zeros((1, self.n_hidden_neurons))\n",
    "        self.b12 = np.zeros((1, self.n_classes))\n",
    "        \n",
    "    def __activation(self, activation_function, X):\n",
    "        if activation_function == \"sigmoid\":\n",
    "            return 1/(1+np.exp(-X))\n",
    "        elif activation_function == \"relu\":\n",
    "            return (X > 0) * X\n",
    "        elif activation_function == \"tanh\":\n",
    "            return (np.exp(X) + np.exp(-X))/(np.exp(X) - np.exp(-X))\n",
    "        \n",
    "    def __activation_derivative(self, activation_function, X):\n",
    "        if activation_function == \"sigmoid\":\n",
    "            return self.__activation(activation_function, X) * (1 - self.__activation(activation_function, X))\n",
    "        elif activation_function == \"relu\":\n",
    "            return X > 0\n",
    "        elif activation_function == \"tanh\":\n",
    "            return 1 - self.__activation(activation_function, X)**2\n",
    "    \n",
    "    def __error(self, preds, ground, error=\"mean\"):\n",
    "        return -np.mean(ground * np.log(preds) + (1 - ground) * np.log(1 - preds))\n",
    "        # return 0.5 * preds.shape[1] * ((ground - preds)**2).sum()\n",
    "        \n",
    "    \n",
    "    #Note: output activation will always be sigmoid\n",
    "    def train(self, epochs=100, lr = 1e-1, hidden_layer_activation = \"relu\", batch_size = 16, thresold = 0.6):\n",
    "        global log\n",
    "        log += f\"{self.n_hidden_neurons}, {batch_size},\"\n",
    "        log += f\"{epochs}, {lr}, {hidden_layer_activation},\"\n",
    "        # forward the data, and then calculate the training accuracy\n",
    "        self.hidden_layer_activation = hidden_layer_activation\n",
    "        print(f\"number of batches {self.X_train.shape[0]//batch_size}\")\n",
    "        error = 0\n",
    "        for epoch in range(epochs):\n",
    "            #batch gd\n",
    "            batches = (self.X_train.shape[0] % batch_size)\n",
    "            exact_batches = True if batches == 0 else False\n",
    "            n_batches = (self.X_train.shape[0]//batch_size) if exact_batches else (self.X_train.shape[0]//batch_size + 1)\n",
    "            for batch in range(n_batches):\n",
    "                b = batch*batch_size\n",
    "                b_1 = self.X_train.shape[0] if (not exact_batches) and (batch == n_batches-1) else (batch+1)*batch_size\n",
    "                self.X_batch = self.X_train[b:b_1]\n",
    "                self.Y_batch = self.y_train[b:b_1]\n",
    "                self.Z01 = self.X_batch.dot(self.W01) + self.b01\n",
    "                self.A01 = self.__activation(hidden_layer_activation, self.Z01)\n",
    "                self.Z02 = self.A01.dot(self.W12) + self.b12\n",
    "                self.A02 = self.__activation(\"sigmoid\", self.Z02)\n",
    "                \n",
    "                error = self.__error(self.A02, self.Y_batch)\n",
    "\n",
    "                self.backward()\n",
    "\n",
    "                self.W12 -= lr * self.A01.T.dot(self.d_error_W12)\n",
    "                self.b12 -= lr * np.sum(self.d_error_W12, axis=0, keepdims = True)\n",
    "                self.W01 -= lr * self.X_batch.T.dot(self.d_error_W01)\n",
    "\n",
    "            # print(f\"Error {error}\")\n",
    "            if epoch % 10 == 0:\n",
    "                self.test(epoch, self.X_train, self.y_train)\n",
    "                print(f\"Error {error}\")\n",
    "                # train_accuracy = self.accuracy(self.A02, self.Y_batch)\n",
    "                # print(self.Y_batch, b, b_1)\n",
    "\n",
    "                \n",
    "        # log += f\"{train_accuracy},\"\n",
    "    def backward(self):\n",
    "        self.d_error_A02 = (self.A02 - self.Y_batch)/len(self.Y_batch)\n",
    "        self.d_error_W12 = (self.d_error_A02) * self.__activation_derivative(\"sigmoid\", self.Z02)\n",
    "        \n",
    "        self.d_error_W01 = (\n",
    "            (self.d_error_W12).dot(self.W12.T) * self.__activation_derivative(self.hidden_layer_activation, self.Z01))\n",
    "    \n",
    "    \n",
    "    def __pre_process(self, embeddings, train_test_ratio = 0.3):\n",
    "        # self.raw_data = self.raw_data.drop(\"Id\",axis=1)\n",
    "        # species_np = np.array(self.raw_data[\"Species\"])\n",
    "        # onehotencoder  = OneHotEncoder(sparse_output = False)\n",
    "        # target_onehot = onehotencoder.fit_transform(species_np.reshape(-1,1))\n",
    "        # self.raw_data=self.raw_data.drop(\"Species\", axis=1)\n",
    "        \n",
    "        X = embeddings \n",
    "        y = self.emotions_onehot\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = X, X, y, y\n",
    "  \n",
    "\n",
    "    def accuracy(self, y_pred, y_ground):\n",
    "        y_train_predicted_classes = np.argmax(y_pred, axis = 1)\n",
    "        y_train_ground_classes = np.argmax(y_ground, axis=1)\n",
    "        accuracy = ((y_train_predicted_classes == y_train_ground_classes).sum())/len(y_train_predicted_classes)\n",
    "        return accuracy\n",
    "               \n",
    "    def test(self, epoch, X, y):\n",
    "        global log\n",
    "        Z01 = X.dot(self.W01) + self.b01\n",
    "        A01 = self.__activation(self.hidden_layer_activation, Z01)\n",
    "        Z02 = A01.dot(self.W12) + self.b12\n",
    "        A02 = self.__activation(\"sigmoid\", Z02)\n",
    "        predictions = A02.round()\n",
    "        precision_metric = precision(y, predictions, y.shape[1])\n",
    "        hamming_loss_metric = hamming_loss(y, predictions)\n",
    "        top3_metric = top2_accuracy(A02, y)\n",
    "        print(f\"Epoch {epoch}, Precision {precision_metric}, top3metric {top3_metric}, Hamming loss {hamming_loss_metric}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        Z01 = X.dot(self.W01) + self.b01\n",
    "        A01 = self.__activation(self.hidden_layer_activation, Z01)\n",
    "        Z02 = A01.dot(self.W12) + self.b12\n",
    "        A02 = self.__activation(\"sigmoid\", Z02)\n",
    "        return A02\n",
    "\n",
    "        \n",
    "    def print_shapes(self):\n",
    "        print(f\"Xtrain shape {self.X_train.shape}\")\n",
    "        print(f\"ytrain shape {self.y_train.shape}\")\n",
    "        print(f\"Xtest shape {self.X_test.shape}\")\n",
    "        print(f\"ytest shape {self.y_test.shape}\")\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"../../data/EdmondsDance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     NaN\n",
       "1     NaN\n",
       "2     NaN\n",
       "3     NaN\n",
       "4     NaN\n",
       "       ..\n",
       "519   NaN\n",
       "520   NaN\n",
       "521   NaN\n",
       "522   NaN\n",
       "523   NaN\n",
       "Name: Unnamed: 11, Length: 524, dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove unwanted columns\n",
    "raw_data.pop(\"Unnamed: 0\")\n",
    "raw_data.pop(\"Unnamed: 11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Song</th>\n",
       "      <th>Artists</th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Trust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Anticipation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apollo</td>\n",
       "      <td>Hardwell, Amba Shepherd</td>\n",
       "      <td>Just one day in the life&lt;br&gt;So I can understan...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lullaby</td>\n",
       "      <td>R3HAB, Mike Williams</td>\n",
       "      <td>Hypnotized, this love out of me&lt;br&gt;Without you...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Melody (Tip Of My Tongue)</td>\n",
       "      <td>Mike Williams</td>\n",
       "      <td>I stand a little too close&lt;br&gt;You stare a litt...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Take Me Home</td>\n",
       "      <td>Cash Cash, Bebe Rexha</td>\n",
       "      <td>I'm falling to pieces&lt;br&gt;But I need this&lt;br&gt;Ye...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>City of Dreams</td>\n",
       "      <td>Dirty South, Alesso</td>\n",
       "      <td>Everything seems like a city of dreams,&lt;br&gt;I n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Song                  Artists  \\\n",
       "0                     Apollo  Hardwell, Amba Shepherd   \n",
       "1                    Lullaby     R3HAB, Mike Williams   \n",
       "2  Melody (Tip Of My Tongue)            Mike Williams   \n",
       "3               Take Me Home    Cash Cash, Bebe Rexha   \n",
       "4             City of Dreams     Dirty South, Alesso    \n",
       "\n",
       "                                              Lyrics  Joy  Trust  Fear  \\\n",
       "0  Just one day in the life<br>So I can understan...    1      1     0   \n",
       "1  Hypnotized, this love out of me<br>Without you...    0      0     1   \n",
       "2  I stand a little too close<br>You stare a litt...    1      1     0   \n",
       "3  I'm falling to pieces<br>But I need this<br>Ye...    0      0     0   \n",
       "4  Everything seems like a city of dreams,<br>I n...    0      0     0   \n",
       "\n",
       "   Surprise  Sadness  Disgust  Anger  Anticipation  \n",
       "0         1        0        0      0             0  \n",
       "1         0        1        0      0             0  \n",
       "2         0        0        0      0             1  \n",
       "3         1        1        1      0             0  \n",
       "4         1        1        0      0             0  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Joy</th>\n",
       "      <th>Trust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Anticipation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>524.000000</td>\n",
       "      <td>524.000000</td>\n",
       "      <td>524.000000</td>\n",
       "      <td>524.000000</td>\n",
       "      <td>524.000000</td>\n",
       "      <td>524.000000</td>\n",
       "      <td>524.000000</td>\n",
       "      <td>524.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.438931</td>\n",
       "      <td>0.561069</td>\n",
       "      <td>0.196565</td>\n",
       "      <td>0.129771</td>\n",
       "      <td>0.353053</td>\n",
       "      <td>0.219466</td>\n",
       "      <td>0.137405</td>\n",
       "      <td>0.475191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.496731</td>\n",
       "      <td>0.496731</td>\n",
       "      <td>0.397780</td>\n",
       "      <td>0.336372</td>\n",
       "      <td>0.478376</td>\n",
       "      <td>0.414280</td>\n",
       "      <td>0.344603</td>\n",
       "      <td>0.499861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Joy       Trust        Fear    Surprise     Sadness     Disgust  \\\n",
       "count  524.000000  524.000000  524.000000  524.000000  524.000000  524.000000   \n",
       "mean     0.438931    0.561069    0.196565    0.129771    0.353053    0.219466   \n",
       "std      0.496731    0.496731    0.397780    0.336372    0.478376    0.414280   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "50%      0.000000    1.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "75%      1.000000    1.000000    0.000000    0.000000    1.000000    0.000000   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "            Anger  Anticipation  \n",
       "count  524.000000    524.000000  \n",
       "mean     0.137405      0.475191  \n",
       "std      0.344603      0.499861  \n",
       "min      0.000000      0.000000  \n",
       "25%      0.000000      0.000000  \n",
       "50%      0.000000      0.000000  \n",
       "75%      0.000000      1.000000  \n",
       "max      1.000000      1.000000  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocab size 3000000\n"
     ]
    }
   ],
   "source": [
    "def load_embedding_model():\n",
    "    \"\"\" Load GloVe Vectors\n",
    "        Return:\n",
    "            wv_from_bin: All 400000 embeddings, each lengh 200\n",
    "    \"\"\"\n",
    "    import gensim.downloader as api\n",
    "    wv_from_bin = api.load(\"word2vec-google-news-300\")\n",
    "    print(\"Loaded vocab size %i\" % len(list(wv_from_bin.index_to_key)))\n",
    "    return wv_from_bin\n",
    "wv_from_bin = load_embedding_model()\n",
    "# wv_from_bin = load_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lyric: str) -> list[str]:\n",
    "    # lowercase the text, remove stop words, punctuation and keep only the words\n",
    "    lyric.replace(\"<br>\", \"\\n\")\n",
    "    tokens = nltk.tokenize.word_tokenize(lyric.lower())\n",
    "    stop_words = stopwords.words(\"english\") + list(string.punctuation)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    alpha_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words]\n",
    "\n",
    "    return alpha_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorise(lyrics: str) -> np.ndarray:\n",
    "    tokens = tokenize(lyrics)\n",
    "    lyric_vector = np.zeros(300)\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            lyric_vector += wv_from_bin.get_vector(token.lower())\n",
    "        except:\n",
    "            continue\n",
    "    return lyric_vector / np.linalg.norm(lyric_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Just one day in the life<br>So I can understan...\n",
       "1      Hypnotized, this love out of me<br>Without you...\n",
       "2      I stand a little too close<br>You stare a litt...\n",
       "3      I'm falling to pieces<br>But I need this<br>Ye...\n",
       "4      Everything seems like a city of dreams,<br>I n...\n",
       "                             ...                        \n",
       "519    ashes to ashes<br>we're falling down<br>so we ...\n",
       "520    I want to hold you<br>I want to hold you<br>I ...\n",
       "521    There's not enough room in here<br>For room fo...\n",
       "522    I see you everywhere<br>I never moved on<br>Wi...\n",
       "523    These stars are, really fireflies that lost th...\n",
       "Name: Lyrics, Length: 524, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[\"Lyrics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [00:01<00:00, 358.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# go through each lyrics, tokenize it, vectorize each word, then combine all of them into single average vector and store it in the list\n",
    "lyrics = raw_data[\"Lyrics\"]\n",
    "lyrics_embeddings = []\n",
    "unsupported_tokens = set()\n",
    "label_embedding_map = {} # dict{str: np.array([])}\n",
    "for lyric in tqdm(lyrics):\n",
    "    lyric_vector = np.zeros(300)\n",
    "    for token in tokenize(lyric):\n",
    "        try:\n",
    "            lyric_vector += wv_from_bin.get_vector(token.lower())\n",
    "        except KeyError as e:\n",
    "            # if the word is not present in the glove then key error is raised, so handle the exception and move on\n",
    "            unsupported_tokens.add(token)\n",
    "            continue\n",
    "    lyrics_embeddings.append(lyric_vector)\n",
    "\n",
    "\n",
    "lyrics_embeddings = np.stack(lyrics_embeddings)\n",
    "scaled_lyrics_embeddings = lyrics_embeddings / np.linalg.norm(lyrics_embeddings, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(raw_data, scaled_lyrics_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches 65\n",
      "Epoch 0, Precision 0.15140561935834013, top3metric 0.7862595419847328, Hamming loss 0.3485209923664122\n",
      "Error 0.5754508802629186\n",
      "Epoch 10, Precision 0.16007040319893995, top3metric 0.7385496183206107, Hamming loss 0.29842557251908397\n",
      "Error 0.5064143343727087\n",
      "Epoch 20, Precision 0.15996496781884603, top3metric 0.7423664122137404, Hamming loss 0.2989026717557252\n",
      "Error 0.5087259206703109\n",
      "Epoch 30, Precision 0.16027862652480718, top3metric 0.7461832061068703, Hamming loss 0.29842557251908397\n",
      "Error 0.5114422207002614\n",
      "Epoch 40, Precision 0.16026708075153362, top3metric 0.75, Hamming loss 0.29842557251908397\n",
      "Error 0.5134101403586184\n",
      "Epoch 50, Precision 0.1611741519350215, top3metric 0.7538167938931297, Hamming loss 0.2967557251908397\n",
      "Error 0.5149822771973102\n",
      "Epoch 60, Precision 0.16054835459714997, top3metric 0.7595419847328244, Hamming loss 0.29770992366412213\n",
      "Error 0.516273799999355\n",
      "Epoch 70, Precision 0.16185717987395776, top3metric 0.7652671755725191, Hamming loss 0.2948473282442748\n",
      "Error 0.517318093940218\n",
      "Epoch 80, Precision 0.16196808637548907, top3metric 0.7652671755725191, Hamming loss 0.29413167938931295\n",
      "Error 0.5181778936128557\n",
      "Epoch 90, Precision 0.16247528714231932, top3metric 0.7633587786259542, Hamming loss 0.29270038167938933\n",
      "Error 0.5189504843089585\n",
      "Epoch 100, Precision 0.1627094927664093, top3metric 0.7652671755725191, Hamming loss 0.2919847328244275\n",
      "Error 0.5194889766337736\n",
      "Epoch 110, Precision 0.16385088343548893, top3metric 0.7729007633587787, Hamming loss 0.28912213740458015\n",
      "Error 0.51977895738591\n",
      "Epoch 120, Precision 0.16496286393345216, top3metric 0.7786259541984732, Hamming loss 0.2862595419847328\n",
      "Error 0.5199814884230402\n",
      "Epoch 130, Precision 0.16661169458669675, top3metric 0.7805343511450382, Hamming loss 0.28196564885496184\n",
      "Error 0.5197596411101666\n",
      "Epoch 140, Precision 0.16721025626989083, top3metric 0.7900763358778626, Hamming loss 0.28029580152671757\n",
      "Error 0.5164478550004303\n",
      "Epoch 150, Precision 0.1684815218753254, top3metric 0.799618320610687, Hamming loss 0.27695610687022904\n",
      "Error 0.5150683122103947\n",
      "Epoch 160, Precision 0.16874661175062483, top3metric 0.8034351145038168, Hamming loss 0.2762404580152672\n",
      "Error 0.5132737143436945\n",
      "Epoch 170, Precision 0.1695795013390355, top3metric 0.816793893129771, Hamming loss 0.2738549618320611\n",
      "Error 0.5114135257531204\n",
      "Epoch 180, Precision 0.17021871657142895, top3metric 0.816793893129771, Hamming loss 0.27218511450381677\n",
      "Error 0.5087038574693316\n",
      "Epoch 190, Precision 0.17068769537786826, top3metric 0.8110687022900763, Hamming loss 0.27099236641221375\n",
      "Error 0.5059994169410424\n",
      "Epoch 200, Precision 0.1701042449796452, top3metric 0.816793893129771, Hamming loss 0.272662213740458\n",
      "Error 0.5025478884945214\n",
      "Epoch 210, Precision 0.17148893733981557, top3metric 0.8206106870229007, Hamming loss 0.26884541984732824\n",
      "Error 0.49903118125033763\n",
      "Epoch 220, Precision 0.17228597816502278, top3metric 0.8263358778625954, Hamming loss 0.26645992366412213\n",
      "Error 0.4947282004808716\n",
      "Epoch 230, Precision 0.1723757789227961, top3metric 0.8396946564885496, Hamming loss 0.2659828244274809\n",
      "Error 0.49142062326241953\n",
      "Epoch 240, Precision 0.17260331076601643, top3metric 0.8416030534351145, Hamming loss 0.2652671755725191\n",
      "Error 0.4875145099771607\n",
      "Epoch 250, Precision 0.17347347979865563, top3metric 0.851145038167939, Hamming loss 0.26264312977099236\n",
      "Error 0.48399632451882674\n",
      "Epoch 260, Precision 0.17466104971984436, top3metric 0.8549618320610687, Hamming loss 0.2593034351145038\n",
      "Error 0.4805892786726495\n",
      "Epoch 270, Precision 0.17533846414924192, top3metric 0.8606870229007634, Hamming loss 0.2573950381679389\n",
      "Error 0.4778133718278102\n",
      "Epoch 280, Precision 0.17601336059560785, top3metric 0.8683206106870229, Hamming loss 0.25524809160305345\n",
      "Error 0.47507605168753053\n",
      "Epoch 290, Precision 0.17656299786338264, top3metric 0.8664122137404581, Hamming loss 0.2535782442748092\n",
      "Error 0.47237751945847634\n",
      "Epoch 300, Precision 0.17710861015814577, top3metric 0.8664122137404581, Hamming loss 0.25190839694656486\n",
      "Error 0.4695865888836623\n",
      "Epoch 310, Precision 0.17877983947458614, top3metric 0.8645038167938931, Hamming loss 0.24689885496183206\n",
      "Error 0.46626621596009743\n",
      "Epoch 320, Precision 0.17925302367838283, top3metric 0.8721374045801527, Hamming loss 0.2452290076335878\n",
      "Error 0.46304492657650453\n",
      "Epoch 330, Precision 0.18065002233472072, top3metric 0.8664122137404581, Hamming loss 0.2411736641221374\n",
      "Error 0.45922877191625616\n",
      "Epoch 340, Precision 0.1796907841983278, top3metric 0.8702290076335878, Hamming loss 0.24379770992366412\n",
      "Error 0.4551950632929811\n",
      "Epoch 350, Precision 0.18018394334065752, top3metric 0.8702290076335878, Hamming loss 0.24236641221374045\n",
      "Error 0.4510558803757467\n",
      "Epoch 360, Precision 0.1807950131719046, top3metric 0.8683206106870229, Hamming loss 0.24045801526717558\n",
      "Error 0.446790631790262\n",
      "Epoch 370, Precision 0.18158126944245034, top3metric 0.8721374045801527, Hamming loss 0.23807251908396945\n",
      "Error 0.44272376450527906\n",
      "Epoch 380, Precision 0.18166589913380202, top3metric 0.8721374045801527, Hamming loss 0.23759541984732824\n",
      "Error 0.4386233187201324\n",
      "Epoch 390, Precision 0.18215241593348708, top3metric 0.8721374045801527, Hamming loss 0.2361641221374046\n",
      "Error 0.43463495867755914\n",
      "Epoch 400, Precision 0.18262245629651608, top3metric 0.8740458015267175, Hamming loss 0.23473282442748092\n",
      "Error 0.43118791033413995\n",
      "Epoch 410, Precision 0.1830095182023354, top3metric 0.8740458015267175, Hamming loss 0.23330152671755724\n",
      "Error 0.4281690336224323\n",
      "Epoch 420, Precision 0.1832264089156984, top3metric 0.8759541984732825, Hamming loss 0.2325858778625954\n",
      "Error 0.42533186763159414\n",
      "Epoch 430, Precision 0.18337071156435347, top3metric 0.8721374045801527, Hamming loss 0.2321087786259542\n",
      "Error 0.4227985278407783\n",
      "Epoch 440, Precision 0.18370751141164715, top3metric 0.8721374045801527, Hamming loss 0.23115458015267176\n",
      "Error 0.42048408843433616\n",
      "Epoch 450, Precision 0.18443794458946788, top3metric 0.8759541984732825, Hamming loss 0.22900763358778625\n",
      "Error 0.4185749388048754\n",
      "Epoch 460, Precision 0.18442043346986314, top3metric 0.8778625954198473, Hamming loss 0.22900763358778625\n",
      "Error 0.4167400916547886\n",
      "Epoch 470, Precision 0.18452706001108712, top3metric 0.8740458015267175, Hamming loss 0.22876908396946566\n",
      "Error 0.41516709383542827\n",
      "Epoch 480, Precision 0.18472322511830103, top3metric 0.8721374045801527, Hamming loss 0.22805343511450382\n",
      "Error 0.41375879522304404\n",
      "Epoch 490, Precision 0.18477685445756054, top3metric 0.8759541984732825, Hamming loss 0.2278148854961832\n",
      "Error 0.41230134379565697\n",
      "Epoch 500, Precision 0.18486747980820178, top3metric 0.8740458015267175, Hamming loss 0.22733778625954199\n",
      "Error 0.411013196202479\n",
      "Epoch 510, Precision 0.185380316932793, top3metric 0.8721374045801527, Hamming loss 0.22566793893129772\n",
      "Error 0.40955815065671486\n",
      "Epoch 520, Precision 0.1859617862159445, top3metric 0.8702290076335878, Hamming loss 0.22399809160305342\n",
      "Error 0.4083563468006746\n",
      "Epoch 530, Precision 0.18578523201810873, top3metric 0.8721374045801527, Hamming loss 0.22447519083969467\n",
      "Error 0.407391847452695\n",
      "Epoch 540, Precision 0.18627677775368082, top3metric 0.8759541984732825, Hamming loss 0.22280534351145037\n",
      "Error 0.4063986645141746\n",
      "Epoch 550, Precision 0.18680175922326478, top3metric 0.8778625954198473, Hamming loss 0.2211354961832061\n",
      "Error 0.40540950031223544\n",
      "Epoch 560, Precision 0.18701062564294624, top3metric 0.8797709923664122, Hamming loss 0.22041984732824427\n",
      "Error 0.40450657818269775\n",
      "Epoch 570, Precision 0.18765864469357538, top3metric 0.8835877862595419, Hamming loss 0.21851145038167938\n",
      "Error 0.4034475407036314\n",
      "Epoch 580, Precision 0.18784688243506714, top3metric 0.8874045801526718, Hamming loss 0.21779580152671757\n",
      "Error 0.4027051021085244\n",
      "Epoch 590, Precision 0.18854331934357918, top3metric 0.8912213740458015, Hamming loss 0.21564885496183206\n",
      "Error 0.40182479830462625\n",
      "Epoch 600, Precision 0.18834785823410174, top3metric 0.8912213740458015, Hamming loss 0.21612595419847327\n",
      "Error 0.40107851153148977\n",
      "Epoch 610, Precision 0.18817256896889062, top3metric 0.8931297709923665, Hamming loss 0.21660305343511452\n",
      "Error 0.40050563940149675\n",
      "Epoch 620, Precision 0.18839752388702344, top3metric 0.8950381679389313, Hamming loss 0.21588740458015268\n",
      "Error 0.40002398983643656\n",
      "Epoch 630, Precision 0.1883670589360888, top3metric 0.8969465648854962, Hamming loss 0.21612595419847327\n",
      "Error 0.39962230077287714\n",
      "Epoch 640, Precision 0.18831004989138764, top3metric 0.8969465648854962, Hamming loss 0.21612595419847327\n",
      "Error 0.39918511116463096\n",
      "Epoch 650, Precision 0.18872140458856324, top3metric 0.8969465648854962, Hamming loss 0.21469465648854963\n",
      "Error 0.3987576008652421\n",
      "Epoch 660, Precision 0.18910068538560912, top3metric 0.898854961832061, Hamming loss 0.21350190839694658\n",
      "Error 0.39838759812051\n",
      "Epoch 670, Precision 0.18937311325542655, top3metric 0.898854961832061, Hamming loss 0.21254770992366412\n",
      "Error 0.3978858251962841\n",
      "Epoch 680, Precision 0.18937311325542655, top3metric 0.898854961832061, Hamming loss 0.21254770992366412\n",
      "Error 0.39732479482577027\n",
      "Epoch 690, Precision 0.1895960877563454, top3metric 0.9026717557251909, Hamming loss 0.21183206106870228\n",
      "Error 0.39677482433785266\n",
      "Epoch 700, Precision 0.1899060387202101, top3metric 0.9045801526717557, Hamming loss 0.21087786259541985\n",
      "Error 0.39626844373101044\n",
      "Epoch 710, Precision 0.1900218988956725, top3metric 0.9122137404580153, Hamming loss 0.21040076335877864\n",
      "Error 0.3957718425858908\n",
      "Epoch 720, Precision 0.1899542528696947, top3metric 0.9103053435114504, Hamming loss 0.21063931297709923\n",
      "Error 0.39529564431552666\n",
      "Epoch 730, Precision 0.19039853710248392, top3metric 0.9103053435114504, Hamming loss 0.20920801526717558\n",
      "Error 0.39475984563153516\n",
      "Epoch 740, Precision 0.19040770238284027, top3metric 0.9103053435114504, Hamming loss 0.20896946564885496\n",
      "Error 0.3942056580643274\n",
      "Epoch 750, Precision 0.1903595088501524, top3metric 0.9122137404580153, Hamming loss 0.20920801526717558\n",
      "Error 0.3937896022334219\n",
      "Epoch 760, Precision 0.19047507063786423, top3metric 0.9141221374045801, Hamming loss 0.20873091603053434\n",
      "Error 0.3933212530054473\n",
      "Epoch 770, Precision 0.19050461915932704, top3metric 0.9122137404580153, Hamming loss 0.20849236641221375\n",
      "Error 0.39286932063676555\n",
      "Epoch 780, Precision 0.19071546878579107, top3metric 0.9141221374045801, Hamming loss 0.20801526717557253\n",
      "Error 0.3923418113126868\n",
      "Epoch 790, Precision 0.1908784450754845, top3metric 0.9179389312977099, Hamming loss 0.2072996183206107\n",
      "Error 0.39192541570667\n",
      "Epoch 800, Precision 0.19131875456289543, top3metric 0.9217557251908397, Hamming loss 0.20586832061068702\n",
      "Error 0.39155987733068587\n",
      "Epoch 810, Precision 0.19164474047885274, top3metric 0.9236641221374046, Hamming loss 0.2049141221374046\n",
      "Error 0.3911534613554902\n",
      "Epoch 820, Precision 0.19195785617405653, top3metric 0.9255725190839694, Hamming loss 0.20372137404580154\n",
      "Error 0.3906740946975239\n",
      "Epoch 830, Precision 0.1920907711162814, top3metric 0.9255725190839694, Hamming loss 0.2032442748091603\n",
      "Error 0.3903102471960497\n",
      "Epoch 840, Precision 0.19252813430059523, top3metric 0.9255725190839694, Hamming loss 0.20181297709923665\n",
      "Error 0.3898523955609914\n",
      "Epoch 850, Precision 0.19270065282949034, top3metric 0.9293893129770993, Hamming loss 0.2013358778625954\n",
      "Error 0.3894988448928145\n",
      "Epoch 860, Precision 0.1930964565924455, top3metric 0.9274809160305344, Hamming loss 0.19990458015267176\n",
      "Error 0.38911538769980963\n",
      "Epoch 870, Precision 0.19333465496895824, top3metric 0.9274809160305344, Hamming loss 0.19918893129770993\n",
      "Error 0.3887944049728026\n",
      "Epoch 880, Precision 0.19340036283157097, top3metric 0.9255725190839694, Hamming loss 0.1989503816793893\n",
      "Error 0.3884109875294154\n",
      "Epoch 890, Precision 0.19383339483936746, top3metric 0.9274809160305344, Hamming loss 0.19775763358778625\n",
      "Error 0.3880378201056912\n",
      "Epoch 900, Precision 0.19372528697590968, top3metric 0.9312977099236641, Hamming loss 0.19799618320610687\n",
      "Error 0.38776073246181636\n",
      "Epoch 910, Precision 0.193790771588577, top3metric 0.9236641221374046, Hamming loss 0.19775763358778625\n",
      "Error 0.38741411410581933\n",
      "Epoch 920, Precision 0.19398702204549062, top3metric 0.9274809160305344, Hamming loss 0.19704198473282442\n",
      "Error 0.3870879291743847\n",
      "Epoch 930, Precision 0.19429058637737678, top3metric 0.9293893129770993, Hamming loss 0.19608778625954199\n",
      "Error 0.38630970902296397\n",
      "Epoch 940, Precision 0.19435576557028938, top3metric 0.9293893129770993, Hamming loss 0.19584923664122136\n",
      "Error 0.38592894437508174\n",
      "Epoch 950, Precision 0.19426929985371183, top3metric 0.9293893129770993, Hamming loss 0.19608778625954199\n",
      "Error 0.3857944713150915\n",
      "Epoch 960, Precision 0.19424822112911336, top3metric 0.9293893129770993, Hamming loss 0.19608778625954199\n",
      "Error 0.3845746808656201\n",
      "Epoch 970, Precision 0.19439966358964875, top3metric 0.9293893129770993, Hamming loss 0.19561068702290077\n",
      "Error 0.3844051169774297\n",
      "Epoch 980, Precision 0.1945511060501841, top3metric 0.9312977099236641, Hamming loss 0.19513358778625955\n",
      "Error 0.38389158523620837\n",
      "Epoch 990, Precision 0.19474615440323823, top3metric 0.9351145038167938, Hamming loss 0.19441793893129772\n",
      "Error 0.3833898037246408\n"
     ]
    }
   ],
   "source": [
    "nn.train(epochs=1000, lr=1e-1, hidden_layer_activation=\"relu\", batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "song = \"\"\"\n",
    "I heard that you're settled down\n",
    "That you found a girl and you're married now\n",
    "I heard that your dreams came true\n",
    "Guess she gave you things, I didn't give to you\n",
    "Old friend, why are you so shy?\n",
    "Ain't like you to hold back or hide from the light\n",
    "I hate to turn up out of the blue, uninvited\n",
    "But I couldn't stay away, I couldn't fight it\n",
    "I had hoped you'd see my face\n",
    "And that you'd be reminded that for me, it isn't over\n",
    "Never mind, I'll find someone like you\n",
    "I wish nothing but the best for you, too\n",
    "\"Don't forget me, \" I beg\n",
    "I remember you said\n",
    "\"Sometimes it lasts in love, but sometimes it hurts instead\"\n",
    "\"Sometimes it lasts in love, but sometimes it hurts instead\"\n",
    "You know how the time flies\n",
    "Only yesterday was the time of our lives\n",
    "We were born and raised in a summer haze\n",
    "Bound by the surprise of our glory days\n",
    "I hate to turn up out of the blue, uninvited\n",
    "But I couldn't stay away, I couldn't fight it\n",
    "I had hoped you'd see my face\n",
    "And that you'd be reminded that for me, it isn't over\n",
    "Never mind, I'll find someone like you\n",
    "I wish nothing but the best for you, too\n",
    "\"Don't forget me, \" I begged\n",
    "I remember you said\n",
    "\"Sometimes it lasts in love, but sometimes it hurts instead\"\n",
    "Nothing compares, no worries or cares\n",
    "Regrets and mistakes, they're memories made\n",
    "Who would have known how bittersweet this would taste?\n",
    "Never mind, I'll find someone like you\n",
    "I wish nothing but the best for you\n",
    "\"Don't forget me, \" I beg\n",
    "I remember you said\n",
    "\"Sometimes it lasts in love, but sometimes it hurts instead\"\n",
    "Never mind, I'll find someone like you\n",
    "I wish nothing but the best for you, too\n",
    "\"Don't forget me, \" I begged\n",
    "I remember you said\n",
    "\"Sometimes it lasts in love, but sometimes it hurts instead\"\n",
    "\"Sometimes it lasts in love, but sometimes it hurts instead\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(lyrics: str) -> str:\n",
    "    song_vector = vectorise(lyrics)[None,:]\n",
    "    return nn.predict(song_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = predict(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02755619, 0.14568433, 0.5158429 , 0.24160766, 0.89812024,\n",
       "        0.66938076, 0.30923268, 0.22966507]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 5, 2, 6, 3, 7, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(probs[0])[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Joy',\n",
       " 'Trust',\n",
       " 'Fear',\n",
       " 'Surprise',\n",
       " 'Sadness',\n",
       " 'Disgust',\n",
       " 'Anger',\n",
       " 'Anticipation']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NeuralNetwork.predict of <__main__.NeuralNetwork object at 0x000002B920210190>>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sadness', 'Disgust', 'Fear', 'Anger', 'Surprise', 'Anticipation',\n",
       "       'Trust', 'Joy'], dtype='<U12')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(nn.labels)[np.argsort(probs[0])[::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"../embeddings/nn.pickle\", \"wb\") as f:\n",
    "    pickle.dump(nn, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open(\"../embeddings/nn.pickle\", \"rb\") as f:\n",
    "    a = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Joy',\n",
       " 'Trust',\n",
       " 'Fear',\n",
       " 'Surprise',\n",
       " 'Sadness',\n",
       " 'Disgust',\n",
       " 'Anger',\n",
       " 'Anticipation']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chess-engine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
