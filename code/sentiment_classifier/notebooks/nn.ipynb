{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\shivs\\anaconda3\\envs\\chess-engine\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import nltk # just for tokenization\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred, num_classes):\n",
    "    # Initialize arrays to store true positives, false positives, and precision\n",
    "    TP = np.zeros(num_classes)\n",
    "    FP = np.zeros(num_classes)\n",
    "    precision_scores = np.zeros(num_classes)\n",
    "\n",
    "    # Calculate true positives and false positives for each class\n",
    "    for i in range(num_classes):\n",
    "        TP[i] = np.sum((y_true == i) & (y_pred == i))\n",
    "        FP[i] = np.sum((y_true != i) & (y_pred == i))\n",
    "\n",
    "    # Compute precision for each class\n",
    "    for i in range(num_classes):\n",
    "        if TP[i] + FP[i] > 0:\n",
    "            precision_scores[i] = TP[i] / (TP[i] + FP[i])\n",
    "\n",
    "    return np.mean(precision_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_loss(y_true, y_pred):\n",
    "    # Calculate number of mismatches\n",
    "    num_mismatches = np.sum(y_true != y_pred)\n",
    "\n",
    "    # Compute Hamming Loss\n",
    "    hamming_loss = num_mismatches / (y_true.shape[0] * y_true.shape[1])\n",
    "\n",
    "    return hamming_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top3_accuracy(predicted_probs, true_labels):\n",
    "\n",
    "    sorted_indices = np.argsort(predicted_probs, axis=1)[:, ::-1]\n",
    "\n",
    "    # Check if true labels are in top-3 predicted labels\n",
    "    top3_correct = np.any(true_labels[np.arange(len(true_labels))[:, None], sorted_indices[:, :3]], axis=1)\n",
    "    # Calculate top-3 accuracy\n",
    "    top3_accuracy = np.mean(top3_correct)\n",
    "    \n",
    "    return top3_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, raw_data, embeddings, hidden_neurons: int =3):\n",
    "        \n",
    "        self.raw_data = raw_data\n",
    "        self.random_state = 42\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = None, None, None, None\n",
    "        self.labels = ['Joy', 'Trust', 'Fear', 'Surprise','Sadness', 'Disgust', 'Anger', 'Anticipation']\n",
    "        self.emotions_onehot = np.array(raw_data.loc[:, self.labels])\n",
    "        self.__pre_process(embeddings)\n",
    "        \n",
    "        self.n_classes = self.y_train.shape[1]\n",
    "        self.n_input_features = self.X_train.shape[1]\n",
    "        self.n_hidden_neurons = hidden_neurons\n",
    "        \n",
    "        \n",
    "        #weights from input layer to hidden layer1\n",
    "        np.random.seed(self.random_state)\n",
    "        self.W01 = np.random.randn(self.n_input_features, self.n_hidden_neurons)\n",
    "        self.W12 = np.random.randn(self.n_hidden_neurons, self.n_classes)\n",
    "        \n",
    "        self.b01 = np.zeros((1, self.n_hidden_neurons))\n",
    "        self.b12 = np.zeros((1, self.n_classes))\n",
    "        \n",
    "    def __activation(self, activation_function, X):\n",
    "        if activation_function == \"sigmoid\":\n",
    "            return 1/(1+np.exp(-X))\n",
    "        elif activation_function == \"relu\":\n",
    "            return (X > 0) * X\n",
    "        elif activation_function == \"tanh\":\n",
    "            return (np.exp(X) + np.exp(-X))/(np.exp(X) - np.exp(-X))\n",
    "        \n",
    "    def __activation_derivative(self, activation_function, X):\n",
    "        if activation_function == \"sigmoid\":\n",
    "            return self.__activation(activation_function, X) * (1 - self.__activation(activation_function, X))\n",
    "        elif activation_function == \"relu\":\n",
    "            return X > 0\n",
    "        elif activation_function == \"tanh\":\n",
    "            return 1 - self.__activation(activation_function, X)**2\n",
    "    \n",
    "    def __error(self, preds, ground, error=\"mean\"):\n",
    "        return -np.mean(ground * np.log(preds) + (1 - ground) * np.log(1 - preds))\n",
    "        # return 0.5 * preds.shape[1] * ((ground - preds)**2).sum()\n",
    "        \n",
    "    \n",
    "    #Note: output activation will always be sigmoid\n",
    "    def train(self, epochs=100, lr = 1e-1, hidden_layer_activation = \"relu\", batch_size = 16, thresold = 0.6):\n",
    "        log_folder = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        summary_writer = tf.summary.create_file_writer(log_folder)\n",
    "       \n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.text(\"Hidden neuron\", str(self.n_hidden_neurons), step=0)\n",
    "            tf.summary.text(\"Batch size\", str(batch_size), step=0)\n",
    "            tf.summary.text(\"Epochs\", str(epochs), step=0)\n",
    "            tf.summary.text(\"Learning rate\", str(lr), step=0)\n",
    "            tf.summary.text(\"Hidden Layer Activation\", hidden_layer_activation, step=0)\n",
    "\n",
    "        # forward the data, and then calculate the training accuracy\n",
    "        self.hidden_layer_activation = hidden_layer_activation\n",
    "        print(f\"number of batches {self.X_train.shape[0]//batch_size}\")\n",
    "        error = 0\n",
    "        for epoch in range(epochs):\n",
    "            #batch gd\n",
    "            batches = (self.X_train.shape[0] % batch_size)\n",
    "            exact_batches = True if batches == 0 else False\n",
    "            n_batches = (self.X_train.shape[0]//batch_size) if exact_batches else (self.X_train.shape[0]//batch_size + 1)\n",
    "            for batch in range(n_batches):\n",
    "                b = batch*batch_size\n",
    "                b_1 = self.X_train.shape[0] if (not exact_batches) and (batch == n_batches-1) else (batch+1)*batch_size\n",
    "                self.X_batch = self.X_train[b:b_1]\n",
    "                self.Y_batch = self.y_train[b:b_1]\n",
    "                self.Z01 = self.X_batch.dot(self.W01) + self.b01\n",
    "                self.A01 = self.__activation(hidden_layer_activation, self.Z01)\n",
    "                self.Z02 = self.A01.dot(self.W12) + self.b12\n",
    "                self.A02 = self.__activation(\"sigmoid\", self.Z02)\n",
    "                \n",
    "                error = self.__error(self.A02, self.Y_batch)\n",
    "\n",
    "                self.backward()\n",
    "\n",
    "                self.W12 -= lr * self.A01.T.dot(self.d_error_W12)\n",
    "                self.b12 -= lr * np.sum(self.d_error_W12, axis=0, keepdims = True)\n",
    "                self.W01 -= lr * self.X_batch.T.dot(self.d_error_W01)\n",
    "\n",
    "            # print(f\"Error {error}\")\n",
    "            if epoch % 10 == 0:\n",
    "                precision_metric, hamming_loss_metric, top3_metric = self.metrics(epoch, self.X_train, self.y_train)\n",
    "\n",
    "                print(f\"Error {error}\")\n",
    "                with summary_writer.as_default():\n",
    "                    tf.summary.scalar(\"Top 3 accuracy\", top3_metric, step=epoch)\n",
    "                    tf.summary.scalar(\"Hamming Loss\", hamming_loss_metric, step=epoch)\n",
    "                    tf.summary.scalar(\"Precision\", precision_metric, step=epoch)\n",
    "                    tf.summary.scalar(\"Loss\", error, step=epoch)\n",
    "                # train_accuracy = self.accuracy(self.A02, self.Y_batch)\n",
    "                # print(self.Y_batch, b, b_1)\n",
    "\n",
    "                \n",
    "        # log += f\"{train_accuracy},\"\n",
    "    def backward(self):\n",
    "        self.d_error_A02 = (self.A02 - self.Y_batch)/len(self.Y_batch)\n",
    "        self.d_error_W12 = (self.d_error_A02) * self.__activation_derivative(\"sigmoid\", self.Z02)\n",
    "        \n",
    "        self.d_error_W01 = (\n",
    "            (self.d_error_W12).dot(self.W12.T) * self.__activation_derivative(self.hidden_layer_activation, self.Z01))\n",
    "    \n",
    "    \n",
    "    def __pre_process(self, embeddings, train_test_ratio = 0.3):\n",
    "        # self.raw_data = self.raw_data.drop(\"Id\",axis=1)\n",
    "        # species_np = np.array(self.raw_data[\"Species\"])\n",
    "        # onehotencoder  = OneHotEncoder(sparse_output = False)\n",
    "        # target_onehot = onehotencoder.fit_transform(species_np.reshape(-1,1))\n",
    "        # self.raw_data=self.raw_data.drop(\"Species\", axis=1)\n",
    "        \n",
    "        X = embeddings \n",
    "        y = self.emotions_onehot\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = X, X, y, y\n",
    "  \n",
    "\n",
    "    def accuracy(self, y_pred, y_ground):\n",
    "        y_train_predicted_classes = np.argmax(y_pred, axis = 1)\n",
    "        y_train_ground_classes = np.argmax(y_ground, axis=1)\n",
    "        accuracy = ((y_train_predicted_classes == y_train_ground_classes).sum())/len(y_train_predicted_classes)\n",
    "        return accuracy\n",
    "               \n",
    "    def metrics(self, epoch, X, y):\n",
    "        Z01 = X.dot(self.W01) + self.b01\n",
    "        A01 = self.__activation(self.hidden_layer_activation, Z01)\n",
    "        Z02 = A01.dot(self.W12) + self.b12\n",
    "        A02 = self.__activation(\"sigmoid\", Z02)\n",
    "        predictions = A02.round()\n",
    "        precision_metric = precision(y, predictions, y.shape[1])\n",
    "        hamming_loss_metric = hamming_loss(y, predictions)\n",
    "        top3_metric = top3_accuracy(A02, y)\n",
    "        return precision_metric, hamming_loss_metric, top3_metric\n",
    "\n",
    "        print(f\"Epoch {epoch}, Precision {precision_metric}, top3metric {top3_metric}, Hamming loss {hamming_loss_metric}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        Z01 = X.dot(self.W01) + self.b01\n",
    "        A01 = self.__activation(self.hidden_layer_activation, Z01)\n",
    "        Z02 = A01.dot(self.W12) + self.b12\n",
    "        A02 = self.__activation(\"sigmoid\", Z02)\n",
    "        return A02\n",
    "\n",
    "        \n",
    "    def print_shapes(self):\n",
    "        print(f\"Xtrain shape {self.X_train.shape}\")\n",
    "        print(f\"ytrain shape {self.y_train.shape}\")\n",
    "        print(f\"Xtest shape {self.X_test.shape}\")\n",
    "        print(f\"ytest shape {self.y_test.shape}\")\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"../../data/EdmondsDance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     NaN\n",
       "1     NaN\n",
       "2     NaN\n",
       "3     NaN\n",
       "4     NaN\n",
       "       ..\n",
       "519   NaN\n",
       "520   NaN\n",
       "521   NaN\n",
       "522   NaN\n",
       "523   NaN\n",
       "Name: Unnamed: 11, Length: 524, dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove unwanted columns\n",
    "raw_data.pop(\"Unnamed: 0\")\n",
    "raw_data.pop(\"Unnamed: 11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Song</th>\n",
       "      <th>Artists</th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Trust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Anticipation</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Apollo</td>\n",
       "      <td>Hardwell, Amba Shepherd</td>\n",
       "      <td>Just one day in the life&lt;br&gt;So I can understan...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Lullaby</td>\n",
       "      <td>R3HAB, Mike Williams</td>\n",
       "      <td>Hypnotized, this love out of me&lt;br&gt;Without you...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Melody (Tip Of My Tongue)</td>\n",
       "      <td>Mike Williams</td>\n",
       "      <td>I stand a little too close&lt;br&gt;You stare a litt...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Take Me Home</td>\n",
       "      <td>Cash Cash, Bebe Rexha</td>\n",
       "      <td>I'm falling to pieces&lt;br&gt;But I need this&lt;br&gt;Ye...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>City of Dreams</td>\n",
       "      <td>Dirty South, Alesso</td>\n",
       "      <td>Everything seems like a city of dreams,&lt;br&gt;I n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                       Song                  Artists  \\\n",
       "0           0                     Apollo  Hardwell, Amba Shepherd   \n",
       "1           1                    Lullaby     R3HAB, Mike Williams   \n",
       "2           2  Melody (Tip Of My Tongue)            Mike Williams   \n",
       "3           3               Take Me Home    Cash Cash, Bebe Rexha   \n",
       "4           4             City of Dreams     Dirty South, Alesso    \n",
       "\n",
       "                                              Lyrics  Joy  Trust  Fear  \\\n",
       "0  Just one day in the life<br>So I can understan...    1      1     0   \n",
       "1  Hypnotized, this love out of me<br>Without you...    0      0     1   \n",
       "2  I stand a little too close<br>You stare a litt...    1      1     0   \n",
       "3  I'm falling to pieces<br>But I need this<br>Ye...    0      0     0   \n",
       "4  Everything seems like a city of dreams,<br>I n...    0      0     0   \n",
       "\n",
       "   Surprise  Sadness  Disgust  Anger  Anticipation  Unnamed: 11  \n",
       "0         1        0        0      0             0          NaN  \n",
       "1         0        1        0      0             0          NaN  \n",
       "2         0        0        0      0             1          NaN  \n",
       "3         1        1        1      0             0          NaN  \n",
       "4         1        1        0      0             0          NaN  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Trust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Anticipation</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>524.000000</td>\n",
       "      <td>524.000000</td>\n",
       "      <td>524.000000</td>\n",
       "      <td>524.000000</td>\n",
       "      <td>524.000000</td>\n",
       "      <td>524.000000</td>\n",
       "      <td>524.000000</td>\n",
       "      <td>524.000000</td>\n",
       "      <td>524.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>261.500000</td>\n",
       "      <td>0.438931</td>\n",
       "      <td>0.561069</td>\n",
       "      <td>0.196565</td>\n",
       "      <td>0.129771</td>\n",
       "      <td>0.353053</td>\n",
       "      <td>0.219466</td>\n",
       "      <td>0.137405</td>\n",
       "      <td>0.475191</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>151.410039</td>\n",
       "      <td>0.496731</td>\n",
       "      <td>0.496731</td>\n",
       "      <td>0.397780</td>\n",
       "      <td>0.336372</td>\n",
       "      <td>0.478376</td>\n",
       "      <td>0.414280</td>\n",
       "      <td>0.344603</td>\n",
       "      <td>0.499861</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>130.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>261.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>392.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>523.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0         Joy       Trust        Fear    Surprise     Sadness  \\\n",
       "count  524.000000  524.000000  524.000000  524.000000  524.000000  524.000000   \n",
       "mean   261.500000    0.438931    0.561069    0.196565    0.129771    0.353053   \n",
       "std    151.410039    0.496731    0.496731    0.397780    0.336372    0.478376   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%    130.750000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "50%    261.500000    0.000000    1.000000    0.000000    0.000000    0.000000   \n",
       "75%    392.250000    1.000000    1.000000    0.000000    0.000000    1.000000   \n",
       "max    523.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "          Disgust       Anger  Anticipation  Unnamed: 11  \n",
       "count  524.000000  524.000000    524.000000          1.0  \n",
       "mean     0.219466    0.137405      0.475191          0.0  \n",
       "std      0.414280    0.344603      0.499861          NaN  \n",
       "min      0.000000    0.000000      0.000000          0.0  \n",
       "25%      0.000000    0.000000      0.000000          0.0  \n",
       "50%      0.000000    0.000000      0.000000          0.0  \n",
       "75%      0.000000    0.000000      1.000000          0.0  \n",
       "max      1.000000    1.000000      1.000000          0.0  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_model():\n",
    "    \"\"\" Load GloVe Vectors\n",
    "        Return:\n",
    "            wv_from_bin: All 400000 embeddings, each lengh 200\n",
    "    \"\"\"\n",
    "    import gensim.downloader as api\n",
    "    wv_from_bin = api.load(\"word2vec-google-news-300\")\n",
    "    print(\"Loaded vocab size %i\" % len(list(wv_from_bin.index_to_key)))\n",
    "    return wv_from_bin\n",
    "wv_from_bin = load_embedding_model()\n",
    "# wv_from_bin = load_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lyric: str) -> list[str]:\n",
    "    # lowercase the text, remove stop words, punctuation and keep only the words\n",
    "    lyric.replace(\"<br>\", \"\\n\")\n",
    "    tokens = nltk.tokenize.word_tokenize(lyric.lower())\n",
    "    stop_words = stopwords.words(\"english\") + list(string.punctuation)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    alpha_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words]\n",
    "\n",
    "    return alpha_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorise(lyrics: str) -> np.ndarray:\n",
    "    tokens = tokenize(lyrics)\n",
    "    lyric_vector = np.zeros(300)\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            lyric_vector += wv_from_bin.get_vector(token.lower())\n",
    "        except:\n",
    "            continue\n",
    "    return lyric_vector / np.linalg.norm(lyric_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Just one day in the life<br>So I can understan...\n",
       "1      Hypnotized, this love out of me<br>Without you...\n",
       "2      I stand a little too close<br>You stare a litt...\n",
       "3      I'm falling to pieces<br>But I need this<br>Ye...\n",
       "4      Everything seems like a city of dreams,<br>I n...\n",
       "                             ...                        \n",
       "519    ashes to ashes<br>we're falling down<br>so we ...\n",
       "520    I want to hold you<br>I want to hold you<br>I ...\n",
       "521    There's not enough room in here<br>For room fo...\n",
       "522    I see you everywhere<br>I never moved on<br>Wi...\n",
       "523    These stars are, really fireflies that lost th...\n",
       "Name: Lyrics, Length: 524, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[\"Lyrics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [00:01<00:00, 358.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# go through each lyrics, tokenize it, vectorize each word, then combine all of them into single average vector and store it in the list\n",
    "lyrics = raw_data[\"Lyrics\"]\n",
    "lyrics_embeddings = []\n",
    "unsupported_tokens = set()\n",
    "label_embedding_map = {} # dict{str: np.array([])}\n",
    "for lyric in tqdm(lyrics):\n",
    "    lyric_vector = np.zeros(300)\n",
    "    for token in tokenize(lyric):\n",
    "        try:\n",
    "            lyric_vector += wv_from_bin.get_vector(token.lower())\n",
    "        except KeyError as e:\n",
    "            # if the word is not present in the glove then key error is raised, so handle the exception and move on\n",
    "            unsupported_tokens.add(token)\n",
    "            continue\n",
    "    lyrics_embeddings.append(lyric_vector)\n",
    "\n",
    "\n",
    "lyrics_embeddings = np.stack(lyrics_embeddings)\n",
    "scaled_lyrics_embeddings = lyrics_embeddings / np.linalg.norm(lyrics_embeddings, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(raw_data, scaled_lyrics_embeddings, hidden_neurons = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches 65\n",
      "Error 0.921805817579666\n",
      "Error 0.8392326854806853\n",
      "Error 0.7625963723271983\n",
      "Error 0.7081498607662808\n",
      "Error 0.6437755012549213\n",
      "Error 0.5528104833063842\n",
      "Error 0.37469251696445893\n",
      "Error 0.35270467615063783\n",
      "Error 0.33355225343760825\n",
      "Error 0.3186027983304763\n",
      "Error 0.30747722591899845\n",
      "Error 0.2989254643800109\n",
      "Error 0.29071230904253986\n",
      "Error 0.28353419591134277\n",
      "Error 0.2771929460384241\n",
      "Error 0.2723232448045164\n",
      "Error 0.26754870971213285\n",
      "Error 0.2629726640096609\n",
      "Error 0.2583533366898932\n",
      "Error 0.25399790634644237\n",
      "Error 0.24955499947528917\n",
      "Error 0.24557512132942733\n",
      "Error 0.24250031068082595\n",
      "Error 0.23936936285043495\n",
      "Error 0.23620545564267792\n",
      "Error 0.2336076410399766\n",
      "Error 0.23082545550595734\n",
      "Error 0.22807367933811307\n",
      "Error 0.2257817441382008\n",
      "Error 0.2233654561529841\n",
      "Error 0.22136545566879323\n",
      "Error 0.21968105639610008\n",
      "Error 0.21804037886972114\n",
      "Error 0.21603424176870395\n",
      "Error 0.2143690601055961\n",
      "Error 0.2124811243361242\n",
      "Error 0.21074000044946797\n",
      "Error 0.20918898144822962\n",
      "Error 0.20742557517528148\n",
      "Error 0.2061388184702771\n",
      "Error 0.20438512422364702\n",
      "Error 0.20323686844813668\n",
      "Error 0.2012752876428745\n",
      "Error 0.19968995688252458\n",
      "Error 0.19847583742624056\n",
      "Error 0.197401946074512\n",
      "Error 0.1956257408805927\n",
      "Error 0.19409356014805787\n",
      "Error 0.19288036103317754\n",
      "Error 0.1910059050141077\n",
      "Error 0.19023123854667673\n",
      "Error 0.18889338698073896\n",
      "Error 0.18705884331035721\n",
      "Error 0.18558340767580478\n",
      "Error 0.1845053061660622\n",
      "Error 0.18339372660926123\n",
      "Error 0.18219104688510945\n",
      "Error 0.1809273397830381\n",
      "Error 0.1783174859658876\n",
      "Error 0.1763914306535824\n",
      "Error 0.17492580441839983\n",
      "Error 0.17347933716434322\n",
      "Error 0.17196775559067445\n",
      "Error 0.1694473698136914\n",
      "Error 0.16783096252162663\n",
      "Error 0.16648859803295835\n",
      "Error 0.1649718409443096\n",
      "Error 0.1631857547759707\n",
      "Error 0.16062560992526875\n",
      "Error 0.1586059036721278\n",
      "Error 0.15632900070604436\n",
      "Error 0.15466350690563016\n",
      "Error 0.15281268493698832\n",
      "Error 0.15134977503118646\n",
      "Error 0.14911058981054487\n",
      "Error 0.14705636771525094\n",
      "Error 0.144776446216401\n",
      "Error 0.14304322045931972\n",
      "Error 0.14152130931789686\n",
      "Error 0.13978984885739085\n",
      "Error 0.13777680721317243\n",
      "Error 0.1365945362164847\n",
      "Error 0.13540843759907908\n",
      "Error 0.13287488945027573\n",
      "Error 0.1319582748972658\n",
      "Error 0.13065770768873736\n",
      "Error 0.1286371231465171\n",
      "Error 0.12744385278850365\n",
      "Error 0.1253047291515657\n",
      "Error 0.12386965159844121\n",
      "Error 0.1220235366534578\n",
      "Error 0.12122086751756166\n",
      "Error 0.12022414544558754\n",
      "Error 0.11794533545612011\n",
      "Error 0.11732044509502439\n",
      "Error 0.11523957272980881\n",
      "Error 0.11317034428968456\n",
      "Error 0.11247416835108703\n",
      "Error 0.11047604175308595\n",
      "Error 0.10911959021218923\n"
     ]
    }
   ],
   "source": [
    "nn.train(epochs=1000, lr=1e-1, hidden_layer_activation=\"relu\", batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "song = \"\"\"\n",
    "I heard that you're settled down\n",
    "That you found a girl and you're married now\n",
    "I heard that your dreams came true\n",
    "Guess she gave you things, I didn't give to you\n",
    "Old friend, why are you so shy?\n",
    "Ain't like you to hold back or hide from the light\n",
    "I hate to turn up out of the blue, uninvited\n",
    "But I couldn't stay away, I couldn't fight it\n",
    "I had hoped you'd see my face\n",
    "And that you'd be reminded that for me, it isn't over\n",
    "Never mind, I'll find someone like you\n",
    "I wish nothing but the best for you, too\n",
    "\"Don't forget me, \" I beg\n",
    "I remember you said\n",
    "\"Sometimes it lasts in love, but sometimes it hurts instead\"\n",
    "\"Sometimes it lasts in love, but sometimes it hurts instead\"\n",
    "You know how the time flies\n",
    "Only yesterday was the time of our lives\n",
    "We were born and raised in a summer haze\n",
    "Bound by the surprise of our glory days\n",
    "I hate to turn up out of the blue, uninvited\n",
    "But I couldn't stay away, I couldn't fight it\n",
    "I had hoped you'd see my face\n",
    "And that you'd be reminded that for me, it isn't over\n",
    "Never mind, I'll find someone like you\n",
    "I wish nothing but the best for you, too\n",
    "\"Don't forget me, \" I begged\n",
    "I remember you said\n",
    "\"Sometimes it lasts in love, but sometimes it hurts instead\"\n",
    "Nothing compares, no worries or cares\n",
    "Regrets and mistakes, they're memories made\n",
    "Who would have known how bittersweet this would taste?\n",
    "Never mind, I'll find someone like you\n",
    "I wish nothing but the best for you\n",
    "\"Don't forget me, \" I beg\n",
    "I remember you said\n",
    "\"Sometimes it lasts in love, but sometimes it hurts instead\"\n",
    "Never mind, I'll find someone like you\n",
    "I wish nothing but the best for you, too\n",
    "\"Don't forget me, \" I begged\n",
    "I remember you said\n",
    "\"Sometimes it lasts in love, but sometimes it hurts instead\"\n",
    "\"Sometimes it lasts in love, but sometimes it hurts instead\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(lyrics: str) -> str:\n",
    "    song_vector = vectorise(lyrics)[None,:]\n",
    "    return nn.predict(song_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = predict(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.39750192e-03, 1.08628360e-04, 1.08387313e-04, 1.65618458e-03,\n",
       "        9.99625572e-01, 8.22359487e-02, 3.25063222e-02, 1.32114848e-01]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 7, 5, 6, 0, 3, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(probs[0])[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Joy',\n",
       " 'Trust',\n",
       " 'Fear',\n",
       " 'Surprise',\n",
       " 'Sadness',\n",
       " 'Disgust',\n",
       " 'Anger',\n",
       " 'Anticipation']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NeuralNetwork.predict of <__main__.NeuralNetwork object at 0x000002B953048700>>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sadness', 'Anticipation', 'Disgust', 'Anger', 'Joy', 'Surprise',\n",
       "       'Trust', 'Fear'], dtype='<U12')"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(nn.labels)[np.argsort(probs[0])[::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"../embeddings/nn.pickle\", \"wb\") as f:\n",
    "    pickle.dump(nn, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open(\"../embeddings/nn.pickle\", \"rb\") as f:\n",
    "    a = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chess-engine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
