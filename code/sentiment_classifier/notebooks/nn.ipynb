{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\shivs\\anaconda3\\envs\\chess-engine\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import nltk # just for tokenization\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred, num_classes):\n",
    "    # Initialize arrays to store true positives, false positives, and precision\n",
    "    TP = np.zeros(num_classes)\n",
    "    FP = np.zeros(num_classes)\n",
    "    precision_scores = np.zeros(num_classes)\n",
    "\n",
    "    # Calculate true positives and false positives for each class\n",
    "    for i in range(num_classes):\n",
    "        TP[i] = np.sum((y_true == i) & (y_pred == i))\n",
    "        FP[i] = np.sum((y_true != i) & (y_pred == i))\n",
    "\n",
    "    # Compute precision for each class\n",
    "    for i in range(num_classes):\n",
    "        if TP[i] + FP[i] > 0:\n",
    "            precision_scores[i] = TP[i] / (TP[i] + FP[i])\n",
    "\n",
    "    return np.mean(precision_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_loss(y_true, y_pred):\n",
    "    # Calculate number of mismatches\n",
    "    num_mismatches = np.sum(y_true != y_pred)\n",
    "\n",
    "    # Compute Hamming Loss\n",
    "    hamming_loss = num_mismatches / (y_true.shape[0] * y_true.shape[1])\n",
    "\n",
    "    return hamming_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top3_accuracy(predicted_probs, true_labels):\n",
    "\n",
    "    sorted_indices = np.argsort(predicted_probs, axis=1)[:, ::-1]\n",
    "\n",
    "    # Check if true labels are in top-3 predicted labels\n",
    "    top3_correct = np.any(true_labels[np.arange(len(true_labels))[:, None], sorted_indices[:, :3]], axis=1)\n",
    "    # Calculate top-3 accuracy\n",
    "    top3_accuracy = np.mean(top3_correct)\n",
    "    \n",
    "    return top3_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, raw_data, embeddings, hidden_neurons, test_df, test_embeddings):\n",
    "        \n",
    "        self.raw_data = raw_data\n",
    "        self.random_state = 42\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = None, None, None, None\n",
    "        self.labels = ['Joy', 'Trust', 'Fear', 'Surprise','Sadness', 'Disgust', 'Anger', 'Anticipation']\n",
    "        self.emotions_onehot = np.array(raw_data.loc[:, self.labels])\n",
    "        self.X_train, self.y_train = embeddings, self.emotions_onehot\n",
    "        self.X_test, self.y_test = test_embeddings, np.array(test_df.loc[:, self.labels])\n",
    "        # self.__pre_process(embeddings)\n",
    "        \n",
    "        self.n_classes = self.y_train.shape[1]\n",
    "        self.n_input_features = self.X_train.shape[1]\n",
    "        self.n_hidden_neurons = hidden_neurons\n",
    "        \n",
    "        \n",
    "        #weights from input layer to hidden layer1\n",
    "        np.random.seed(self.random_state)\n",
    "        self.W01 = np.random.randn(self.n_input_features, self.n_hidden_neurons)\n",
    "        self.W12 = np.random.randn(self.n_hidden_neurons, self.n_classes)\n",
    "        \n",
    "        self.b01 = np.zeros((1, self.n_hidden_neurons))\n",
    "        self.b12 = np.zeros((1, self.n_classes))\n",
    "        \n",
    "    def __activation(self, activation_function, X):\n",
    "        if activation_function == \"sigmoid\":\n",
    "            return 1/(1+np.exp(-X))\n",
    "        elif activation_function == \"relu\":\n",
    "            return (X > 0) * X\n",
    "        elif activation_function == \"tanh\":\n",
    "            return (np.exp(X) + np.exp(-X))/(np.exp(X) - np.exp(-X))\n",
    "        \n",
    "    def __activation_derivative(self, activation_function, X):\n",
    "        if activation_function == \"sigmoid\":\n",
    "            return self.__activation(activation_function, X) * (1 - self.__activation(activation_function, X))\n",
    "        elif activation_function == \"relu\":\n",
    "            return X > 0\n",
    "        elif activation_function == \"tanh\":\n",
    "            return 1 - self.__activation(activation_function, X)**2\n",
    "    \n",
    "    def __error(self, preds, ground, error=\"mean\"):\n",
    "        return -np.mean(ground * np.log(preds) + (1 - ground) * np.log(1 - preds))\n",
    "        # return 0.5 * preds.shape[1] * ((ground - preds)**2).sum()\n",
    "        \n",
    "    \n",
    "    #Note: output activation will always be sigmoid\n",
    "    def train(self, epochs=100, lr = 1e-1, hidden_layer_activation = \"relu\", batch_size = 16, thresold = 0.6):\n",
    "        log_folder = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        summary_writer = tf.summary.create_file_writer(log_folder)\n",
    "       \n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.text(\"Hidden neuron\", str(self.n_hidden_neurons), step=0)\n",
    "            tf.summary.text(\"Batch size\", str(batch_size), step=0)\n",
    "            tf.summary.text(\"Epochs\", str(epochs), step=0)\n",
    "            tf.summary.text(\"Learning rate\", str(lr), step=0)\n",
    "            tf.summary.text(\"Hidden Layer Activation\", hidden_layer_activation, step=0)\n",
    "\n",
    "        # forward the data, and then calculate the training accuracy\n",
    "        self.hidden_layer_activation = hidden_layer_activation\n",
    "        print(f\"number of batches {self.X_train.shape[0]//batch_size}\")\n",
    "        train_error = 0\n",
    "        for epoch in range(epochs):\n",
    "            #batch gd\n",
    "            batches = (self.X_train.shape[0] % batch_size)\n",
    "            exact_batches = True if batches == 0 else False\n",
    "            n_batches = (self.X_train.shape[0]//batch_size) if exact_batches else (self.X_train.shape[0]//batch_size + 1)\n",
    "            for batch in range(n_batches):\n",
    "                b = batch*batch_size\n",
    "                b_1 = self.X_train.shape[0] if (not exact_batches) and (batch == n_batches-1) else (batch+1)*batch_size\n",
    "                self.X_batch = self.X_train[b:b_1]\n",
    "                self.Y_batch = self.y_train[b:b_1]\n",
    "                self.Z01 = self.X_batch.dot(self.W01) + self.b01\n",
    "                self.A01 = self.__activation(hidden_layer_activation, self.Z01)\n",
    "                self.Z02 = self.A01.dot(self.W12) + self.b12\n",
    "                self.A02 = self.__activation(\"sigmoid\", self.Z02)\n",
    "                \n",
    "                train_error = self.__error(self.A02, self.Y_batch)\n",
    "\n",
    "                self.backward()\n",
    "\n",
    "                self.W12 -= lr * self.A01.T.dot(self.d_error_W12)\n",
    "                self.b12 -= lr * np.sum(self.d_error_W12, axis=0, keepdims = True)\n",
    "                self.W01 -= lr * self.X_batch.T.dot(self.d_error_W01)\n",
    "\n",
    "            # print(f\"Error {error}\")\n",
    "            if epoch % 10 == 0:\n",
    "                test_error, precision_metric, hamming_loss_metric, top3_metric = self.test(epoch)\n",
    "\n",
    "                with summary_writer.as_default():\n",
    "                    tf.summary.scalar(\"Top 3 accuracy\", top3_metric, step=epoch)\n",
    "                    tf.summary.scalar(\"Hamming Loss\", hamming_loss_metric, step=epoch)\n",
    "                    tf.summary.scalar(\"Precision\", precision_metric, step=epoch)\n",
    "                    tf.summary.scalar(\"Loss/Test\", test_error, step=epoch)\n",
    "                    tf.summary.scalar(\"Loss/Train\", train_error, step=epoch)\n",
    "                print(f\"Epoch {epoch}, Train error {train_error}, Test error {test_error}, Precision {precision_metric}, top3metric {top3_metric}, Hamming loss {hamming_loss_metric}\")\n",
    "                # train_accuracy = self.accuracy(self.A02, self.Y_batch)\n",
    "                # print(self.Y_batch, b, b_1)\n",
    "\n",
    "                \n",
    "        # log += f\"{train_accuracy},\"\n",
    "    def backward(self):\n",
    "        self.d_error_A02 = (self.A02 - self.Y_batch)/len(self.Y_batch)\n",
    "        self.d_error_W12 = (self.d_error_A02) * self.__activation_derivative(\"sigmoid\", self.Z02)\n",
    "        \n",
    "        self.d_error_W01 = (\n",
    "            (self.d_error_W12).dot(self.W12.T) * self.__activation_derivative(self.hidden_layer_activation, self.Z01))\n",
    "    \n",
    "    \n",
    "    # def __pre_process(self, embeddings, train_test_ratio = 0.3):\n",
    "    #     # self.raw_data = self.raw_data.drop(\"Id\",axis=1)\n",
    "    #     # species_np = np.array(self.raw_data[\"Species\"])\n",
    "    #     # onehotencoder  = OneHotEncoder(sparse_output = False)\n",
    "    #     # target_onehot = onehotencoder.fit_transform(species_np.reshape(-1,1))\n",
    "    #     # self.raw_data=self.raw_data.drop(\"Species\", axis=1)\n",
    "        \n",
    "    #     X = embeddings \n",
    "    #     y = self.emotions_onehot\n",
    "        \n",
    "    #     self.X_train, self.X_test, self.y_train, self.y_test = X, X, y, y\n",
    "  \n",
    "\n",
    "    def accuracy(self, y_pred, y_ground):\n",
    "        y_train_predicted_classes = np.argmax(y_pred, axis = 1)\n",
    "        y_train_ground_classes = np.argmax(y_ground, axis=1)\n",
    "        accuracy = ((y_train_predicted_classes == y_train_ground_classes).sum())/len(y_train_predicted_classes)\n",
    "        return accuracy\n",
    "               \n",
    "    def test(self, epoch):\n",
    "        X, y = self.X_test, self.y_test\n",
    "        Z01 = X.dot(self.W01) + self.b01\n",
    "        A01 = self.__activation(self.hidden_layer_activation, Z01)\n",
    "        Z02 = A01.dot(self.W12) + self.b12\n",
    "        A02 = self.__activation(\"sigmoid\", Z02)\n",
    "        predictions = A02.round()\n",
    "        error = self.__error(A02, y)\n",
    "        precision_metric = precision(y, predictions, y.shape[1])\n",
    "        hamming_loss_metric = hamming_loss(y, predictions)\n",
    "        top3_metric = top3_accuracy(A02, y)\n",
    "        return error, precision_metric, hamming_loss_metric, top3_metric\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        Z01 = X.dot(self.W01) + self.b01\n",
    "        A01 = self.__activation(self.hidden_layer_activation, Z01)\n",
    "        Z02 = A01.dot(self.W12) + self.b12\n",
    "        A02 = self.__activation(\"sigmoid\", Z02)\n",
    "        return A02\n",
    "\n",
    "        \n",
    "    def print_shapes(self):\n",
    "        print(f\"Xtrain shape {self.X_train.shape}\")\n",
    "        print(f\"ytrain shape {self.y_train.shape}\")\n",
    "        print(f\"Xtest shape {self.X_test.shape}\")\n",
    "        print(f\"ytest shape {self.y_test.shape}\")\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"../../data/EdmondsDance.csv\")\n",
    "train_data = pd.read_csv(\"../../data/train.csv\")\n",
    "train_data = train_data.rename(columns={\"lyrics\":\"Lyrics\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     NaN\n",
       "1     NaN\n",
       "2     NaN\n",
       "3     NaN\n",
       "4     NaN\n",
       "       ..\n",
       "519   NaN\n",
       "520   NaN\n",
       "521   NaN\n",
       "522   NaN\n",
       "523   NaN\n",
       "Name: Unnamed: 11, Length: 524, dtype: float64"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove unwanted columns\n",
    "test_data.pop(\"Unnamed: 0\")\n",
    "test_data.pop(\"Unnamed: 11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Song</th>\n",
       "      <th>Artists</th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Trust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Anticipation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apollo</td>\n",
       "      <td>Hardwell, Amba Shepherd</td>\n",
       "      <td>Just one day in the life&lt;br&gt;So I can understan...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lullaby</td>\n",
       "      <td>R3HAB, Mike Williams</td>\n",
       "      <td>Hypnotized, this love out of me&lt;br&gt;Without you...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Melody (Tip Of My Tongue)</td>\n",
       "      <td>Mike Williams</td>\n",
       "      <td>I stand a little too close&lt;br&gt;You stare a litt...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Take Me Home</td>\n",
       "      <td>Cash Cash, Bebe Rexha</td>\n",
       "      <td>I'm falling to pieces&lt;br&gt;But I need this&lt;br&gt;Ye...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>City of Dreams</td>\n",
       "      <td>Dirty South, Alesso</td>\n",
       "      <td>Everything seems like a city of dreams,&lt;br&gt;I n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Song                  Artists  \\\n",
       "0                     Apollo  Hardwell, Amba Shepherd   \n",
       "1                    Lullaby     R3HAB, Mike Williams   \n",
       "2  Melody (Tip Of My Tongue)            Mike Williams   \n",
       "3               Take Me Home    Cash Cash, Bebe Rexha   \n",
       "4             City of Dreams     Dirty South, Alesso    \n",
       "\n",
       "                                              Lyrics  Joy  Trust  Fear  \\\n",
       "0  Just one day in the life<br>So I can understan...    1      1     0   \n",
       "1  Hypnotized, this love out of me<br>Without you...    0      0     1   \n",
       "2  I stand a little too close<br>You stare a litt...    1      1     0   \n",
       "3  I'm falling to pieces<br>But I need this<br>Ye...    0      0     0   \n",
       "4  Everything seems like a city of dreams,<br>I n...    0      0     0   \n",
       "\n",
       "   Surprise  Sadness  Disgust  Anger  Anticipation  \n",
       "0         1        0        0      0             0  \n",
       "1         0        1        0      0             0  \n",
       "2         0        0        0      0             1  \n",
       "3         1        1        1      0             0  \n",
       "4         1        1        0      0             0  "
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Joy</th>\n",
       "      <th>Trust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Anticipation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>524.000000</td>\n",
       "      <td>524.000000</td>\n",
       "      <td>524.000000</td>\n",
       "      <td>524.000000</td>\n",
       "      <td>524.000000</td>\n",
       "      <td>524.000000</td>\n",
       "      <td>524.000000</td>\n",
       "      <td>524.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.438931</td>\n",
       "      <td>0.561069</td>\n",
       "      <td>0.196565</td>\n",
       "      <td>0.129771</td>\n",
       "      <td>0.353053</td>\n",
       "      <td>0.219466</td>\n",
       "      <td>0.137405</td>\n",
       "      <td>0.475191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.496731</td>\n",
       "      <td>0.496731</td>\n",
       "      <td>0.397780</td>\n",
       "      <td>0.336372</td>\n",
       "      <td>0.478376</td>\n",
       "      <td>0.414280</td>\n",
       "      <td>0.344603</td>\n",
       "      <td>0.499861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Joy       Trust        Fear    Surprise     Sadness     Disgust  \\\n",
       "count  524.000000  524.000000  524.000000  524.000000  524.000000  524.000000   \n",
       "mean     0.438931    0.561069    0.196565    0.129771    0.353053    0.219466   \n",
       "std      0.496731    0.496731    0.397780    0.336372    0.478376    0.414280   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "50%      0.000000    1.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "75%      1.000000    1.000000    0.000000    0.000000    1.000000    0.000000   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "            Anger  Anticipation  \n",
       "count  524.000000    524.000000  \n",
       "mean     0.137405      0.475191  \n",
       "std      0.344603      0.499861  \n",
       "min      0.000000      0.000000  \n",
       "25%      0.000000      0.000000  \n",
       "50%      0.000000      0.000000  \n",
       "75%      0.000000      1.000000  \n",
       "max      1.000000      1.000000  "
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_model():\n",
    "    \"\"\" Load GloVe Vectors\n",
    "        Return:\n",
    "            wv_from_bin: All 400000 embeddings, each lengh 200\n",
    "    \"\"\"\n",
    "    import gensim.downloader as api\n",
    "    wv_from_bin = api.load(\"word2vec-google-news-300\")\n",
    "    print(\"Loaded vocab size %i\" % len(list(wv_from_bin.index_to_key)))\n",
    "    return wv_from_bin\n",
    "wv_from_bin = load_embedding_model()\n",
    "# wv_from_bin = load_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lyric: str) -> list[str]:\n",
    "    # lowercase the text, remove stop words, punctuation and keep only the words\n",
    "    lyric.replace(\"<br>\", \"\\n\")\n",
    "    tokens = nltk.tokenize.word_tokenize(lyric.lower())\n",
    "    stop_words = stopwords.words(\"english\") + list(string.punctuation)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    alpha_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words]\n",
    "\n",
    "    return alpha_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorise(lyrics: str) -> np.ndarray:\n",
    "    tokens = tokenize(lyrics)\n",
    "    lyric_vector = np.zeros(300)\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            lyric_vector += wv_from_bin.get_vector(token.lower())\n",
    "        except:\n",
    "            continue\n",
    "    return lyric_vector / np.linalg.norm(lyric_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through each lyrics, tokenize it, vectorize each word, then combine all of them into single average vector and store it in the list\n",
    "\n",
    "def get_embeddings(raw_data):\n",
    "    lyrics = raw_data[\"Lyrics\"]\n",
    "    lyrics_embeddings = []\n",
    "    unsupported_tokens = set()\n",
    "    label_embedding_map = {} # dict{str: np.array([])}\n",
    "    for lyric in tqdm(lyrics):\n",
    "        lyric_vector = np.zeros(300)\n",
    "        for token in tokenize(lyric):\n",
    "            try:\n",
    "                lyric_vector += wv_from_bin.get_vector(token.lower())\n",
    "            except KeyError as e:\n",
    "                # if the word is not present in the glove then key error is raised, so handle the exception and move on\n",
    "                unsupported_tokens.add(token)\n",
    "                continue\n",
    "        lyrics_embeddings.append(lyric_vector)\n",
    "\n",
    "\n",
    "    lyrics_embeddings = np.stack(lyrics_embeddings)\n",
    "    scaled_lyrics_embeddings = lyrics_embeddings / np.linalg.norm(lyrics_embeddings, axis=1, keepdims=True)\n",
    "    return scaled_lyrics_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [00:01<00:00, 318.65it/s]\n",
      "100%|██████████| 1753/1753 [00:06<00:00, 250.65it/s]\n"
     ]
    }
   ],
   "source": [
    "train_embeddings = get_embeddings(train_data)\n",
    "test_embeddings = get_embeddings(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  raw_data, embeddings, hidden_neurons, test_df, test_embeddings\n",
    "nn = NeuralNetwork(train_data, train_embeddings, hidden_neurons = 32, test_df = test_data, test_embeddings = test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches 109\n",
      "Epoch 0, Train error 0.841289734872073, Test error 1.4004723838201174, Precision 0.1220323499695907, top3metric 0.7251908396946565, Hamming loss 0.47638358778625955\n",
      "Epoch 10, Train error 0.7111049222524194, Test error 1.3633648740214201, Precision 0.13152355603626373, top3metric 0.7366412213740458, Hamming loss 0.37333015267175573\n",
      "Epoch 20, Train error 0.6759011354175272, Test error 1.4055191513710896, Precision 0.12949696336145294, top3metric 0.7232824427480916, Hamming loss 0.3857347328244275\n",
      "Epoch 30, Train error 0.6113661662405083, Test error 1.41248228831901, Precision 0.12827800724568716, top3metric 0.7175572519083969, Hamming loss 0.3916984732824427\n",
      "Epoch 40, Train error 0.5354974573558602, Test error 1.4029939163891672, Precision 0.12687555922637483, top3metric 0.7080152671755725, Hamming loss 0.3974236641221374\n",
      "Epoch 50, Train error 0.46028956928076453, Test error 1.3891544899082826, Precision 0.12561382686744305, top3metric 0.7022900763358778, Hamming loss 0.4026717557251908\n",
      "Epoch 60, Train error 0.3948583659297815, Test error 1.3746877302854923, Precision 0.1253497194219289, top3metric 0.7041984732824428, Hamming loss 0.4036259541984733\n",
      "Epoch 70, Train error 0.34102879228451055, Test error 1.3593447879984302, Precision 0.12552072998293978, top3metric 0.6946564885496184, Hamming loss 0.4017175572519084\n",
      "Epoch 80, Train error 0.2971379244837477, Test error 1.3448371038423979, Precision 0.12568740743550716, top3metric 0.6946564885496184, Hamming loss 0.3995706106870229\n",
      "Epoch 90, Train error 0.26295150325043903, Test error 1.3319596804379747, Precision 0.12584286477475332, top3metric 0.6927480916030534, Hamming loss 0.3969465648854962\n",
      "Epoch 100, Train error 0.2356936154616306, Test error 1.320576565931762, Precision 0.12749056331160036, top3metric 0.6965648854961832, Hamming loss 0.39002862595419846\n",
      "Epoch 110, Train error 0.21464379720100515, Test error 1.3102481497664453, Precision 0.12956237942958518, top3metric 0.6965648854961832, Hamming loss 0.3814408396946565\n",
      "Epoch 120, Train error 0.1982147693660724, Test error 1.300101627119333, Precision 0.12998537397748752, top3metric 0.700381679389313, Hamming loss 0.37905534351145037\n",
      "Epoch 130, Train error 0.18581123614291264, Test error 1.2895366641421029, Precision 0.13083757046534764, top3metric 0.7061068702290076, Hamming loss 0.37595419847328243\n",
      "Epoch 140, Train error 0.17549365228686592, Test error 1.2788823069470965, Precision 0.1319597056780828, top3metric 0.7061068702290076, Hamming loss 0.37189885496183206\n",
      "Epoch 150, Train error 0.16666202558455712, Test error 1.2676947702483718, Precision 0.1332551230621035, top3metric 0.7099236641221374, Hamming loss 0.36712786259541985\n",
      "Epoch 160, Train error 0.15874762162445888, Test error 1.256467335237578, Precision 0.13400227829538638, top3metric 0.7080152671755725, Hamming loss 0.3642652671755725\n",
      "Epoch 170, Train error 0.15207533731952536, Test error 1.245715859613827, Precision 0.13437199405369166, top3metric 0.7099236641221374, Hamming loss 0.36259541984732824\n",
      "Epoch 180, Train error 0.14606940475386937, Test error 1.2353362715326417, Precision 0.1356954894946754, top3metric 0.7118320610687023, Hamming loss 0.35877862595419846\n",
      "Epoch 190, Train error 0.140324885629859, Test error 1.2253219111316287, Precision 0.13704206241519676, top3metric 0.7156488549618321, Hamming loss 0.3554389312977099\n",
      "Epoch 200, Train error 0.1350886108749167, Test error 1.2152823727695892, Precision 0.13790581185778153, top3metric 0.7213740458015268, Hamming loss 0.35353053435114506\n",
      "Epoch 210, Train error 0.13008041374104923, Test error 1.2053218218562693, Precision 0.13938015603202103, top3metric 0.7232824427480916, Hamming loss 0.3501908396946565\n",
      "Epoch 220, Train error 0.12553904221379936, Test error 1.1950513837841714, Precision 0.13986555548254742, top3metric 0.7270992366412213, Hamming loss 0.3487595419847328\n",
      "Epoch 230, Train error 0.12177101670748088, Test error 1.1846104629058878, Precision 0.14045945085785377, top3metric 0.7270992366412213, Hamming loss 0.34780534351145037\n",
      "Epoch 240, Train error 0.11808623058216355, Test error 1.1737401193749815, Precision 0.14148170652024789, top3metric 0.7251908396946565, Hamming loss 0.3451812977099237\n",
      "Epoch 250, Train error 0.11463642545180337, Test error 1.1621944295732314, Precision 0.14182161476041524, top3metric 0.7309160305343512, Hamming loss 0.34422709923664124\n",
      "Epoch 260, Train error 0.11142922858042795, Test error 1.1496705882291385, Precision 0.14163634328950883, top3metric 0.732824427480916, Hamming loss 0.3449427480916031\n",
      "Epoch 270, Train error 0.10864593882207005, Test error 1.135610611888371, Precision 0.14156714746075238, top3metric 0.7366412213740458, Hamming loss 0.34541984732824427\n",
      "Epoch 280, Train error 0.10598749527626608, Test error 1.1195611749478245, Precision 0.14209583024077055, top3metric 0.7385496183206107, Hamming loss 0.34422709923664124\n",
      "Epoch 290, Train error 0.10354037839608034, Test error 1.1010981286567687, Precision 0.14258384844322344, top3metric 0.7423664122137404, Hamming loss 0.34303435114503816\n",
      "Epoch 300, Train error 0.10141230387611629, Test error 1.0794567010258218, Precision 0.14303500033809108, top3metric 0.7461832061068703, Hamming loss 0.3416030534351145\n",
      "Epoch 310, Train error 0.09967071936463662, Test error 1.0541682201909652, Precision 0.14322577830322472, top3metric 0.7461832061068703, Hamming loss 0.34088740458015265\n",
      "Epoch 320, Train error 0.09835800606569706, Test error 1.0264249688916867, Precision 0.1433020860457715, top3metric 0.7480916030534351, Hamming loss 0.34064885496183206\n",
      "Epoch 330, Train error 0.09717474808529472, Test error 0.9996302639088345, Precision 0.1436450245875623, top3metric 0.7480916030534351, Hamming loss 0.33969465648854963\n",
      "Epoch 340, Train error 0.09598127679656764, Test error 0.9779260495626341, Precision 0.14379907768794722, top3metric 0.75, Hamming loss 0.3392175572519084\n",
      "Epoch 350, Train error 0.09458864293542169, Test error 0.9612501947524216, Precision 0.14421460383806559, top3metric 0.7480916030534351, Hamming loss 0.33826335877862596\n",
      "Epoch 360, Train error 0.09303492454851062, Test error 0.9477681417772107, Precision 0.14443914012093859, top3metric 0.7519083969465649, Hamming loss 0.3377862595419847\n",
      "Epoch 370, Train error 0.09149873919498391, Test error 0.9361702004985394, Precision 0.1438246394044357, top3metric 0.75, Hamming loss 0.33969465648854963\n",
      "Epoch 380, Train error 0.08989200034277343, Test error 0.9256315054197921, Precision 0.1437843426634744, top3metric 0.7557251908396947, Hamming loss 0.3399332061068702\n",
      "Epoch 390, Train error 0.08833304942373937, Test error 0.9157979350157864, Precision 0.14378018944859167, top3metric 0.7557251908396947, Hamming loss 0.3401717557251908\n",
      "Epoch 400, Train error 0.08684086290437547, Test error 0.9066788738371845, Precision 0.1441833259126943, top3metric 0.7538167938931297, Hamming loss 0.33945610687022904\n",
      "Epoch 410, Train error 0.08535473328014424, Test error 0.8982106113905086, Precision 0.14429354442349396, top3metric 0.7538167938931297, Hamming loss 0.3392175572519084\n",
      "Epoch 420, Train error 0.08396465294735928, Test error 0.8903701230460079, Precision 0.14414321323650978, top3metric 0.7557251908396947, Hamming loss 0.33969465648854963\n",
      "Epoch 430, Train error 0.08257394611415644, Test error 0.8830537153446416, Precision 0.14402486212534632, top3metric 0.7576335877862596, Hamming loss 0.34041030534351147\n",
      "Epoch 440, Train error 0.08126439836290772, Test error 0.8765380639070043, Precision 0.14380051250867149, top3metric 0.7557251908396947, Hamming loss 0.3413645038167939\n",
      "Epoch 450, Train error 0.0800623101517482, Test error 0.8706774626366363, Precision 0.14412672081813246, top3metric 0.7557251908396947, Hamming loss 0.34064885496183206\n",
      "Epoch 460, Train error 0.07885449618438829, Test error 0.8653276893754912, Precision 0.14452494634746876, top3metric 0.7595419847328244, Hamming loss 0.33969465648854963\n",
      "Epoch 470, Train error 0.07753877950639793, Test error 0.8602784809014131, Precision 0.14448581738655758, top3metric 0.7576335877862596, Hamming loss 0.3399332061068702\n",
      "Epoch 480, Train error 0.07623673122396046, Test error 0.855700345153358, Precision 0.1444470077229397, top3metric 0.7557251908396947, Hamming loss 0.3401717557251908\n",
      "Epoch 490, Train error 0.07501055052460634, Test error 0.8514031251635602, Precision 0.1443739609802317, top3metric 0.7557251908396947, Hamming loss 0.34041030534351147\n",
      "Epoch 500, Train error 0.073883002045283, Test error 0.8474563251061801, Precision 0.14440851421701914, top3metric 0.7557251908396947, Hamming loss 0.34041030534351147\n",
      "Epoch 510, Train error 0.07277359368868191, Test error 0.8437908950837988, Precision 0.14451581129549107, top3metric 0.7538167938931297, Hamming loss 0.3401717557251908\n",
      "Epoch 520, Train error 0.07172728555431683, Test error 0.8403918522445354, Precision 0.14444300506124905, top3metric 0.7519083969465649, Hamming loss 0.34041030534351147\n",
      "Epoch 530, Train error 0.0707007434911184, Test error 0.8372294263499309, Precision 0.14440488179158872, top3metric 0.7538167938931297, Hamming loss 0.34064885496183206\n",
      "Epoch 540, Train error 0.06970818367024359, Test error 0.834270621505594, Precision 0.14407918867089842, top3metric 0.7538167938931297, Hamming loss 0.34184160305343514\n",
      "Epoch 550, Train error 0.06870664559558665, Test error 0.8315093138322854, Precision 0.14390000447698625, top3metric 0.7576335877862596, Hamming loss 0.3425572519083969\n",
      "Epoch 560, Train error 0.06774380295298829, Test error 0.8289326787010209, Precision 0.1437933652737041, top3metric 0.7576335877862596, Hamming loss 0.34303435114503816\n",
      "Epoch 570, Train error 0.06686675204179782, Test error 0.8265180652932809, Precision 0.14365195085729715, top3metric 0.7557251908396947, Hamming loss 0.3435114503816794\n",
      "Epoch 580, Train error 0.06594339303692862, Test error 0.8241725872980492, Precision 0.14372259456216663, top3metric 0.7576335877862596, Hamming loss 0.34327290076335876\n",
      "Epoch 590, Train error 0.06503029532504946, Test error 0.8220475200629205, Precision 0.14397051127581634, top3metric 0.7576335877862596, Hamming loss 0.3425572519083969\n",
      "Epoch 600, Train error 0.06417750765486896, Test error 0.8201224075738517, Precision 0.14386419237223597, top3metric 0.7576335877862596, Hamming loss 0.34303435114503816\n",
      "Epoch 610, Train error 0.06334427340068954, Test error 0.8182856794746667, Precision 0.14411029507951012, top3metric 0.7576335877862596, Hamming loss 0.34279580152671757\n",
      "Epoch 620, Train error 0.06256575221189388, Test error 0.8166090250808786, Precision 0.14411029507951012, top3metric 0.7614503816793893, Hamming loss 0.34279580152671757\n",
      "Epoch 630, Train error 0.06183105265143343, Test error 0.815071932837456, Precision 0.1439011422304096, top3metric 0.7633587786259542, Hamming loss 0.34375\n",
      "Epoch 640, Train error 0.061095564064268526, Test error 0.8137162341315257, Precision 0.14400660991570083, top3metric 0.7614503816793893, Hamming loss 0.34375\n",
      "Epoch 650, Train error 0.06036907643440353, Test error 0.8124697465228389, Precision 0.1442840298434423, top3metric 0.7614503816793893, Hamming loss 0.34303435114503816\n",
      "Epoch 660, Train error 0.05967692079516889, Test error 0.8113436085950694, Precision 0.14418046188432454, top3metric 0.7633587786259542, Hamming loss 0.3435114503816794\n",
      "Epoch 670, Train error 0.05901779069531172, Test error 0.8102995088168063, Precision 0.14418046188432454, top3metric 0.7690839694656488, Hamming loss 0.3435114503816794\n",
      "Epoch 680, Train error 0.05833898712220763, Test error 0.8092940174093906, Precision 0.14438768793280496, top3metric 0.7690839694656488, Hamming loss 0.34303435114503816\n",
      "Epoch 690, Train error 0.05767654483107895, Test error 0.8083803854220754, Precision 0.14431875025282448, top3metric 0.7690839694656488, Hamming loss 0.34327290076335876\n",
      "Epoch 700, Train error 0.05704395272602328, Test error 0.8075019530636164, Precision 0.1441812287479538, top3metric 0.7709923664122137, Hamming loss 0.34375\n",
      "Epoch 710, Train error 0.05645851195764024, Test error 0.8067033325075135, Precision 0.14397582506293713, top3metric 0.7709923664122137, Hamming loss 0.34446564885496184\n",
      "Epoch 720, Train error 0.05589026777554697, Test error 0.8059817510225749, Precision 0.14360380445472654, top3metric 0.7729007633587787, Hamming loss 0.3458969465648855\n",
      "Epoch 730, Train error 0.055361133876929036, Test error 0.8053163924345613, Precision 0.14363940959486388, top3metric 0.7729007633587787, Hamming loss 0.3458969465648855\n",
      "Epoch 740, Train error 0.05485905794600814, Test error 0.8047099300245998, Precision 0.1435048215851164, top3metric 0.7729007633587787, Hamming loss 0.3463740458015267\n",
      "Epoch 750, Train error 0.05437815523664949, Test error 0.8041632210317671, Precision 0.14371429748101922, top3metric 0.7729007633587787, Hamming loss 0.3461354961832061\n",
      "Epoch 760, Train error 0.0539154720375084, Test error 0.8037238448055157, Precision 0.14378507860409256, top3metric 0.7748091603053435, Hamming loss 0.3461354961832061\n",
      "Epoch 770, Train error 0.0534545452746253, Test error 0.8032967742111243, Precision 0.14371842073080462, top3metric 0.7748091603053435, Hamming loss 0.3463740458015267\n",
      "Epoch 780, Train error 0.0530069403015334, Test error 0.8028803904768392, Precision 0.14385563999351328, top3metric 0.7786259541984732, Hamming loss 0.3461354961832061\n",
      "Epoch 790, Train error 0.052562780455566664, Test error 0.8024950299215241, Precision 0.14402390934007267, top3metric 0.7786259541984732, Hamming loss 0.34565839694656486\n",
      "Epoch 800, Train error 0.0521396454554233, Test error 0.8021522097708892, Precision 0.14395732011772874, top3metric 0.7786259541984732, Hamming loss 0.3458969465648855\n",
      "Epoch 810, Train error 0.05171872777357625, Test error 0.8018003432042001, Precision 0.14399237126509853, top3metric 0.7786259541984732, Hamming loss 0.3458969465648855\n",
      "Epoch 820, Train error 0.05130837587576858, Test error 0.8014986497435429, Precision 0.14375820302156167, top3metric 0.7862595419847328, Hamming loss 0.34661259541984735\n",
      "Epoch 830, Train error 0.050903218775469314, Test error 0.8012513911539711, Precision 0.14375820302156167, top3metric 0.7862595419847328, Hamming loss 0.34661259541984735\n",
      "Epoch 840, Train error 0.05049865086306423, Test error 0.801047159998526, Precision 0.14385970806376266, top3metric 0.7862595419847328, Hamming loss 0.3463740458015267\n",
      "Epoch 850, Train error 0.05010693237731076, Test error 0.8009194827542142, Precision 0.14393003038605995, top3metric 0.7900763358778626, Hamming loss 0.3463740458015267\n",
      "Epoch 860, Train error 0.04971877778390178, Test error 0.8008201386468343, Precision 0.14389922335011646, top3metric 0.7900763358778626, Hamming loss 0.34661259541984735\n",
      "Epoch 870, Train error 0.04932864101809865, Test error 0.8006942905489038, Precision 0.14376776133220284, top3metric 0.7900763358778626, Hamming loss 0.34708969465648853\n",
      "Epoch 880, Train error 0.04896024968043858, Test error 0.8005805572145615, Precision 0.14370218822273617, top3metric 0.7881679389312977, Hamming loss 0.3473282442748092\n",
      "Epoch 890, Train error 0.048605135862898906, Test error 0.8004951278237322, Precision 0.14380306959607186, top3metric 0.7900763358778626, Hamming loss 0.34708969465648853\n",
      "Epoch 900, Train error 0.048245331452714445, Test error 0.8004091229615453, Precision 0.14363671991443785, top3metric 0.7900763358778626, Hamming loss 0.3475667938931298\n",
      "Epoch 910, Train error 0.04791239257481904, Test error 0.8003283786881354, Precision 0.14367221100877586, top3metric 0.7900763358778626, Hamming loss 0.3475667938931298\n",
      "Epoch 920, Train error 0.04759442820735905, Test error 0.8002644077918174, Precision 0.14360693793970358, top3metric 0.7900763358778626, Hamming loss 0.34780534351145037\n",
      "Epoch 930, Train error 0.047254128165136064, Test error 0.8002126072025233, Precision 0.14367221100877586, top3metric 0.7881679389312977, Hamming loss 0.3475667938931298\n",
      "Epoch 940, Train error 0.04691631519634471, Test error 0.8001844406264342, Precision 0.14354176856147483, top3metric 0.7900763358778626, Hamming loss 0.34804389312977096\n",
      "Epoch 950, Train error 0.04660578176148542, Test error 0.8001884904945941, Precision 0.14351241110555124, top3metric 0.7900763358778626, Hamming loss 0.3482824427480916\n",
      "Epoch 960, Train error 0.04632283289070841, Test error 0.8002412968160291, Precision 0.14338276582186638, top3metric 0.7919847328244275, Hamming loss 0.3487595419847328\n",
      "Epoch 970, Train error 0.04605550871093646, Test error 0.8002900214701886, Precision 0.14338276582186638, top3metric 0.7938931297709924, Hamming loss 0.3487595419847328\n",
      "Epoch 980, Train error 0.045779826978776625, Test error 0.8003258666164411, Precision 0.14349010560389241, top3metric 0.7938931297709924, Hamming loss 0.3487595419847328\n",
      "Epoch 990, Train error 0.04551477452652445, Test error 0.8003654955419351, Precision 0.1433613940467033, top3metric 0.7938931297709924, Hamming loss 0.34923664122137404\n"
     ]
    }
   ],
   "source": [
    "nn.train(epochs=1000, lr=1e-2, hidden_layer_activation=\"relu\", batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error 0.8004127657235502, Precision 0.14349717189735994, top3metric 0.7958015267175572, Hamming loss 0.34899809160305345\n"
     ]
    }
   ],
   "source": [
    "test_error, precision_score, hamming_score, top3_accuracy = nn.test(1) #error, precision, hamming loss, top3 accuracy\n",
    "print(f\"Test error {test_error}, Precision {precision_score}, top3metric {top3_accuracy}, Hamming loss {hamming_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "song = \"\"\"\n",
    "La, la la la la la, la la la\n",
    "La, la la la la la, la la la\n",
    "La, la la la la la, la la la\n",
    "La, la la la la la, la la la\n",
    "Hold on to me\n",
    "Don't let me go\n",
    "Who cares what they see?\n",
    "Who cares what they know?\n",
    "Your first name is Free\n",
    "Last name is Dom\n",
    "We choose to believe\n",
    "In where we're from\n",
    "Man's red flower\n",
    "It's in every living thing\n",
    "Mind, use your power\n",
    "Spirit, use your wings\n",
    "Freedom!\n",
    "Freedom!\n",
    "Freedom!\n",
    "Freedom\n",
    "Freedom\n",
    "Freedom\n",
    "Hold on to me\n",
    "Don't let me go\n",
    "Cheetahs need to eat\n",
    "Run, antelope\n",
    "Your first name is King\n",
    "Last name is Dom\n",
    "'Cause you still believe\n",
    "In everyone\n",
    "When a baby first breathes\n",
    "When night sees sunrise\n",
    "When the whale hunts the sea\n",
    "When man recognizes\n",
    "Freedom!\n",
    "Freedom!\n",
    "Freedom!\n",
    "Freedom\n",
    "Freedom\n",
    "Breathe in\n",
    "We are from heat\n",
    "The electric one\n",
    "Does it shock you to see\n",
    "He left us the sun?\n",
    "Atoms in the air\n",
    "Organisms in the sea\n",
    "Sun and yes, night\n",
    "Are made of the same things\n",
    "Freedom!\n",
    "Freedom!\n",
    "Freedom!\n",
    "Freedom\n",
    "Freedom\n",
    "Freedom\n",
    "Freedom\n",
    "Freedom\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(lyrics: str) -> str:\n",
    "    song_vector = vectorise(lyrics)[None,:]\n",
    "    return nn.predict(song_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = predict(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.72160929, 0.10660481, 0.36424165, 0.20913482, 0.13555264,\n",
       "        0.02070832, 0.44212887, 0.26779786]])"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Joy', 'Anger', 'Fear', 'Anticipation', 'Surprise', 'Sadness',\n",
       "       'Trust', 'Disgust'], dtype='<U12')"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(nn.labels)[np.argsort(probs[0])[::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"../embeddings/nn.pickle\", \"wb\") as f:\n",
    "    pickle.dump(nn, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open(\"../embeddings/nn.pickle\", \"rb\") as f:\n",
    "    a = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating labels for spotify dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chess-engine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
